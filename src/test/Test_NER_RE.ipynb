{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnkLz9cZB2js",
        "outputId": "befedf1f-5526-4a37-8f23-983fad694961",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.17.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UuPqiysXzZT_",
        "outputId": "9fe99d41-d5e8-4dac-a9cb-3b077953ea79"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.17.0\n",
            "  Downloading tensorflow-2.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.17.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.17.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.17.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.17.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.17.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.17.0) (3.13.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.17.0) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.17.0) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.17.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.17.0) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.17.0)\n",
            "  Downloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.17.0) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.17.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.17.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.17.0) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.17.0) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.17.0) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.17.0) (1.71.0)\n",
            "Collecting tensorboard<2.18,>=2.17 (from tensorflow==2.17.0)\n",
            "  Downloading tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.17.0) (3.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.17.0) (0.37.1)\n",
            "Collecting numpy<2.0.0,>=1.23.5 (from tensorflow==2.17.0)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.17.0) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->tensorflow==2.17.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->tensorflow==2.17.0) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->tensorflow==2.17.0) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow==2.17.0) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->tensorflow==2.17.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->tensorflow==2.17.0) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow==2.17.0) (0.1.2)\n",
            "Downloading tensorflow-2.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.6-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, numpy, tensorboard, tensorflow\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.17.0 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.6 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.6 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.17.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.17.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4 protobuf-4.25.6 tensorboard-2.17.1 tensorflow-2.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iE1HMgDZ4klE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "from transformers import AutoTokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from transformers import TFBertModel\n",
        "from tensorflow.keras import layers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tag_dict = {\n",
        "    'B-GENE-Y': 1, 'I-GENE-Y': 2,\n",
        "    'B-GENE-N': 3, 'I-GENE-N': 4,\n",
        "    'B-CHEMICAL': 5, 'I-CHEMICAL': 6\n",
        "}\n",
        "\n",
        "tag_dict_map = {\n",
        "    (0,): 0, (0, 1): 1, (0, 2, 3, 5): 2,\n",
        "    (0, 4): 3, (0, 1, 6): 4, (0, 2, 6): 5, (0, 2): 6, (0, 3, 6): 7,\n",
        "    (0, 5): 8, (0, 4, 6): 9, (0, 2, 4, 6): 10, (0, 2, 4): 11,\n",
        "    (0, 5, 6): 12, (0, 3): 13, (0, 6): 14, (0, 1, 5): 15, (0, 2, 5): 16, (0, 3, 5): 17, (0, 4, 5): 18\n",
        "}\n",
        "\n",
        "con = {2:[2,5,6,10,11,16], 4:[3,9,10,11,18], 6:[4,5,7,9,10,12,14]}\n",
        "\n",
        "next_entity = {0:[[0],[]], 1:[con[2],[]], 2:[con[4],con[6]], 3:[[3],[]], 4:[con[2],[]], 5:[[5],[]], 6:[[6],[]], 7:[con[4],[]], 8:[con[6],[]], 9:[[9],[]] ,10:[[10],[]], 11:[[11],[]], 12:[con[6],[]], 13:[con[4],[]], 14:[[14],[]], 15:[con[2],con[6]], 16:[con[6],[]], 17:[con[4],con[6]], 18:[con[6],[]]}\n",
        "\n",
        "r_dict = {\n",
        "    'CPR:0': 0, 'CPR:1': 1, 'CPR:2': 2, 'CPR:3': 3,\n",
        "    'CPR:4': 4, 'CPR:5': 5, 'CPR:6': 6, 'CPR:7': 7,\n",
        "    'CPR:8': 8, 'CPR:9': 9, 'CPR:10': 10\n",
        "}\n",
        "\n",
        "batch_size = 30\n",
        "num_batches = 131\n",
        "\n",
        "num_tags = 19\n",
        "hidden_dim = 64\n",
        "seq_len = 128\n",
        "num_relations = 12\n"
      ],
      "metadata": {
        "id": "c5tPDoZC9-4X"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tf.config.run_functions_eagerly(True)\n",
        "\n",
        "\n",
        "class CustomCRF(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_tags, seq_len, **kwargs):\n",
        "        super(CustomCRF, self).__init__(**kwargs)\n",
        "        self.num_tags = num_tags\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def build(self):\n",
        "        self.start_transitions = self.add_weight(\n",
        "            shape=(self.num_tags,)\n",
        "        )\n",
        "        self.transition_matrix = self.add_weight(\n",
        "            shape=(self.num_tags, self.num_tags)\n",
        "        )\n",
        "        self.end_transitions = self.add_weight(\n",
        "            shape=(self.num_tags,)\n",
        "        )\n",
        "    @tf.function\n",
        "    def call(self, inputs, labels=None, training=None):\n",
        "        emissions, attention_mask = inputs\n",
        "        seq_len = tf.shape(emissions)[1]\n",
        "\n",
        "        if training:\n",
        "            return self._crf_loss(emissions, labels, attention_mask)\n",
        "        else:\n",
        "            return self.viterbi(emissions, attention_mask)\n",
        "\n",
        "    @tf.function\n",
        "    def viterbi(self, emissions, attention_mask):\n",
        "        batch_size = tf.shape(emissions)[0]\n",
        "        seq_len = tf.shape(emissions)[1]\n",
        "        num_tags = tf.shape(emissions)[2]\n",
        "\n",
        "        dp = tf.TensorArray(dtype=tf.float32, size=seq_len, clear_after_read=False)\n",
        "        backpointer = tf.TensorArray(dtype=tf.int32, size=seq_len, clear_after_read=False)\n",
        "\n",
        "        first_step = self.start_transitions + emissions[:, 0, :]\n",
        "        dp = dp.write(0, first_step)\n",
        "\n",
        "        t = tf.constant(1)\n",
        "\n",
        "        def loop_body(t, dp, backpointer):\n",
        "            prev_scores = dp.read(t - 1)\n",
        "            scores = tf.expand_dims(prev_scores, axis=2) + self.transition_matrix\n",
        "            best_scores = tf.reduce_max(scores, axis=1)\n",
        "            best_paths = tf.argmax(scores, axis=1, output_type=tf.int32)\n",
        "            current_scores = emissions[:, t, :] + best_scores\n",
        "            dp = dp.write(t, current_scores)\n",
        "            backpointer = backpointer.write(t, best_paths)\n",
        "            return t + 1, dp, backpointer\n",
        "\n",
        "        _, dp, backpointer = tf.while_loop(\n",
        "            cond=lambda t, *_: t < seq_len,\n",
        "            body=loop_body,\n",
        "            loop_vars=(t, dp, backpointer)\n",
        "        )\n",
        "\n",
        "        last_step_scores = dp.read(seq_len - 1) + self.end_transitions\n",
        "        last_tag = tf.argmax(last_step_scores, axis=1, output_type=tf.int32)\n",
        "\n",
        "        def backtrack_fn(i):\n",
        "            best_path = tf.TensorArray(dtype=tf.int32, size=seq_len, clear_after_read=False)\n",
        "            best_path = best_path.write(seq_len - 1, last_tag[i])\n",
        "            t = seq_len - 2\n",
        "\n",
        "            def backtrack_body(t, best_path):\n",
        "                next_tag = best_path.read(t + 1)\n",
        "                best_tag = backpointer.read(t + 1)[i, next_tag]\n",
        "                best_path = best_path.write(t, best_tag)\n",
        "                return t - 1, best_path\n",
        "\n",
        "            _, best_path = tf.while_loop(\n",
        "                cond=lambda t, *_: t >= 0,\n",
        "                body=backtrack_body,\n",
        "                loop_vars=(t, best_path)\n",
        "            )\n",
        "\n",
        "            return best_path.stack()\n",
        "\n",
        "        best_paths = tf.map_fn(backtrack_fn, tf.range(batch_size), fn_output_signature=tf.int32)\n",
        "        return best_paths\n",
        "\n",
        "\n",
        "class NERModel(tf.keras.Model):\n",
        "    def __init__(self, num_tags, hidden_dim, seq_len, **kwargs):\n",
        "        super(NERModel, self).__init__(**kwargs)\n",
        "        self.bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "        self.lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hidden_dim, return_sequences=True))\n",
        "        self.lstm.build((None, None, self.bert.config.hidden_size))\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(num_tags)\n",
        "        self.dense.build((None, 2*hidden_dim))\n",
        "\n",
        "        self.num_tags = num_tags\n",
        "        self.metrics_dict = {i: tf.Variable([0, 0, 0, 0], dtype=tf.int32) for i in range(num_tags)}\n",
        "        self.crf = CustomCRF(num_tags, seq_len)\n",
        "        self.crf.build()\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs, labels=None, training=None):\n",
        "        bert_output = self.bert(inputs['input_ids'], attention_mask=inputs['attention_mask'])['last_hidden_state']\n",
        "        lstm_output = self.lstm(bert_output)\n",
        "        logits = self.dense(lstm_output)\n",
        "        if training:\n",
        "            return logits\n",
        "        else:\n",
        "            return self.crf((logits, inputs['attention_mask']), training=False)\n",
        "\n",
        "    def reset_metrics(self):\n",
        "        for key in self.metrics_dict:\n",
        "            self.metrics_dict[key].assign([0, 0, 0, 0])\n",
        "\n",
        "    def test_step(self, data, validation=False, inference = False):\n",
        "        if inference:\n",
        "            predictions = self(data, training=False)\n",
        "            return predictions\n",
        "        inputs, labels = data\n",
        "        emissions = self(inputs, training=validation)\n",
        "        if validation:\n",
        "            loss = self.crf._crf_loss(emissions, labels, inputs['attention_mask'], tf.shape(emissions)[1])\n",
        "            return {'loss':loss}\n",
        "        predictions = tf.reshape(emissions, [-1])\n",
        "        true_labels = tf.reshape(labels, [-1])\n",
        "\n",
        "        for i in range(self.num_tags):\n",
        "            true_positives = tf.reduce_sum(tf.cast((predictions == i) & (true_labels == i), tf.int32))\n",
        "            false_positives = tf.reduce_sum(tf.cast((predictions == i) & (true_labels != i), tf.int32))\n",
        "            false_negatives = tf.reduce_sum(tf.cast((predictions != i) & (true_labels == i), tf.int32))\n",
        "            true_negatives = tf.reduce_sum(tf.cast((predictions != i) & (true_labels != i), tf.int32))\n",
        "\n",
        "            self.metrics_dict[i].assign_add([true_positives, true_negatives, false_positives, false_negatives])\n",
        "\n",
        "\n",
        "class RelationExtractionModel(tf.keras.Model):\n",
        "    def __init__(self, ner_model, num_relations, hidden_dim, dropout_rate=0.2):\n",
        "        super(RelationExtractionModel, self).__init__()\n",
        "        self.bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "        self.bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hidden_dim, return_sequences=True))\n",
        "        self.bilstm.build((None, None, ner_model.bert.config.hidden_size))\n",
        "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "        self.layer_norm.build((None,None,2*hidden_dim))\n",
        "        self.dense3_re = tf.keras.layers.Dense(num_relations, activation='softmax')\n",
        "        self.dense3_re.build((None, 4*hidden_dim))\n",
        "        self.num_relations = num_relations\n",
        "        self.metrics_dict = {i: tf.Variable([0, 0, 0, 0], dtype=tf.int32) for i in range(num_relations)}\n",
        "    @tf.function\n",
        "    def call(self, inputs, training=False):\n",
        "        input_ids = inputs['input_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "        ner_tags = inputs['ner_tags']\n",
        "        re_mask = inputs['re_mask']\n",
        "\n",
        "        bert_output = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        token_embeddings = bert_output.last_hidden_state\n",
        "        lstm_output = self.layer_norm(self.bilstm(token_embeddings, training=training))\n",
        "        combined_embeddings = self.extract_entity_pairs(lstm_output, ner_tags, re_mask)\n",
        "        combined_embeddings_stacked = tf.stack(combined_embeddings)\n",
        "        return self.dense3_re(combined_embeddings_stacked)\n",
        "\n",
        "        '''def return_zeros():\n",
        "            return tf.zeros((1, self.dense.units))\n",
        "\n",
        "        def proceed_with_embeddings():\n",
        "            combined_embeddings_stacked = tf.stack(combined_embeddings)\n",
        "            return self.dense(self.dense1(combined_embeddings_stacked))\n",
        "\n",
        "        return tf.cond(\n",
        "            tf.equal(tf.shape(combined_embeddings)[0], 0),\n",
        "            return_zeros,\n",
        "            proceed_with_embeddings\n",
        "        )'''\n",
        "\n",
        "    @tf.function\n",
        "    def extract_entity_pairs(self, lstm_output, ner_tags, re_mask):\n",
        "        combined_embeddings = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "        batch_size = tf.shape(ner_tags)[0]\n",
        "\n",
        "        def process_batch(i, embeddings_list):\n",
        "            re_pairs = re_mask[i]\n",
        "            valid_pairs = tf.boolean_mask(re_pairs, tf.not_equal(re_pairs[:, 0], -1))\n",
        "\n",
        "            def process_pair(j, embeddings_list):\n",
        "                e1_sidx, e1_eidx, relation_type, e2_sidx, e2_eidx = tf.unstack(valid_pairs[j])\n",
        "                e1_emb = self.pool_entity(lstm_output[i], ner_tags[i], e1_sidx, e1_eidx)\n",
        "                e2_emb = self.pool_entity(lstm_output[i], ner_tags[i], e2_sidx, e2_eidx)\n",
        "                combined = tf.concat([e1_emb, e2_emb], axis=-1)\n",
        "                return j + 1, embeddings_list.write(embeddings_list.size(), combined)\n",
        "\n",
        "            _, embeddings_list = tf.while_loop(\n",
        "                lambda j, _: j < tf.shape(valid_pairs)[0],\n",
        "                process_pair,\n",
        "                loop_vars=[0, embeddings_list]\n",
        "            )\n",
        "            return i + 1, embeddings_list\n",
        "\n",
        "        _, final_embeddings = tf.while_loop(\n",
        "            lambda i, _: i < batch_size,\n",
        "            process_batch,\n",
        "            loop_vars=[0, combined_embeddings]\n",
        "        )\n",
        "\n",
        "        return final_embeddings.stack()\n",
        "    @tf.function\n",
        "    def pool_entity(self, lstm_output, ner_tags, start_idx, end_idx):\n",
        "        entity_span = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "\n",
        "        def condition(idx, _):\n",
        "            return idx <= end_idx\n",
        "\n",
        "        def body(idx, entity_span):\n",
        "            entity_span = entity_span.write(entity_span.size(), lstm_output[idx])\n",
        "            return idx + 1, entity_span\n",
        "\n",
        "        _, collected_span = tf.while_loop(\n",
        "            condition, body, loop_vars=[start_idx, entity_span]\n",
        "        )\n",
        "\n",
        "        return tf.reduce_mean(collected_span.stack(), axis=0)\n",
        "\n",
        "    @tf.function\n",
        "    def extract_relation_labels(self, re_mask):\n",
        "        reduced_re_mask = re_mask[:, :, 2]\n",
        "        valid_mask = tf.not_equal(reduced_re_mask, -1)\n",
        "        valid_relation_labels = tf.boolean_mask(reduced_re_mask, valid_mask)\n",
        "        return valid_relation_labels\n",
        "\n",
        "\n",
        "    def reset_metrics(self):\n",
        "        for key in self.metrics_dict:\n",
        "            self.metrics_dict[key].assign([0, 0, 0, 0])\n",
        "\n",
        "    def test_step(self, inputs, validation = False, inference = False):\n",
        "        if tf.size(inputs['re_mask']) == 0 or all(tf.size(mask) == 0 for mask in inputs['re_mask']):\n",
        "            return tf.constant([])\n",
        "        logits = self(inputs, training=False)\n",
        "        predictions = tf.argmax(logits, axis=-1)\n",
        "        if inference:\n",
        "            return predictions\n",
        "        true_labels = self.extract_relation_labels(inputs['re_mask'])\n",
        "        if validation :\n",
        "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(\n",
        "                true_labels, logits\n",
        "            ))\n",
        "            return {'loss':loss}\n",
        "\n",
        "        for i in range(self.num_relations):\n",
        "            tp = tf.reduce_sum(tf.cast((predictions == i) & (true_labels == i), tf.int32))\n",
        "            fp = tf.reduce_sum(tf.cast((predictions == i) & (true_labels != i), tf.int32))\n",
        "            fn = tf.reduce_sum(tf.cast((predictions != i) & (true_labels == i), tf.int32))\n",
        "            tn = tf.reduce_sum(tf.cast((predictions != i) & (true_labels != i), tf.int32))\n",
        "\n",
        "            self.metrics_dict[i].assign_add([tp, tn, fp, fn])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ShSQXyEl8YEI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_model = NERModel(num_tags=num_tags, hidden_dim=128, seq_len=seq_len)\n",
        "ner_model.build(1)\n",
        "\n",
        "ner_model.load_weights('/content/ner_model.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-_vV5vK5lfw",
        "outputId": "2120195d-28d4-44c2-87d7-18f04cda228f",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re_model = RelationExtractionModel(ner_model=ner_model, hidden_dim=hidden_dim, num_relations=num_relations)\n",
        "re_model.build(1)\n",
        "re_model.load_weights('/content/re_model.keras')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3UgcQo6-m6v",
        "outputId": "cf11935f-4ceb-4cee-a686-d2a305dad660",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "ds = load_dataset(\"bigbio/chemprot\", \"chemprot_full_source\")\n"
      ],
      "metadata": {
        "id": "QVVqjnKSMqJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def gen(i, j, ner_tags_segment, next_tags):\n",
        "    l, m = i + 1, j + 1\n",
        "    tem = []\n",
        "    while ner_tags_segment[l] in next_tags[ner_tags_segment[i]][0]:\n",
        "        l += 1\n",
        "    while ner_tags_segment[m] in next_tags[ner_tags_segment[j]][0]:\n",
        "        m += 1\n",
        "    if m - 1 != j and l - 1 != i:\n",
        "        tem.append((i, l - 1, j, m - 1))\n",
        "    l, m = i + 1, j + 1\n",
        "    while ner_tags_segment[l] in next_tags[ner_tags_segment[i]][0]:\n",
        "        l += 1\n",
        "    while ner_tags_segment[m] in next_tags[ner_tags_segment[j]][1]:\n",
        "        m += 1\n",
        "    if m - 1 != j and l - 1 != i:\n",
        "        tem.append((i, l - 1, j, m - 1))\n",
        "    l, m = i + 1, j + 1\n",
        "    while ner_tags_segment[l] in next_tags[ner_tags_segment[i]][1]:\n",
        "        l += 1\n",
        "    while ner_tags_segment[m] in next_tags[ner_tags_segment[j]][0]:\n",
        "        m += 1\n",
        "    if m - 1 != j and l - 1 != i:\n",
        "        tem.append((i, l - 1, j, m - 1))\n",
        "    l, m = i + 1, j + 1\n",
        "    while ner_tags_segment[l] in next_tags[ner_tags_segment[i]][1]:\n",
        "        l += 1\n",
        "    while ner_tags_segment[m] in next_tags[ner_tags_segment[j]][1]:\n",
        "        m += 1\n",
        "    if m - 1 != j and l - 1 != i:\n",
        "        tem.append((i, l - 1, j, m - 1))\n",
        "    return tem\n",
        "\n",
        "def prepare_data(row, tag_dict, r_dict, tag_dict_map, next_tags, max_length=128, stride=10, padding=True):\n",
        "    text = row['text']\n",
        "    entities = row['entities']\n",
        "    relations = row['relations']\n",
        "\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        return_offsets_mapping=True,\n",
        "        return_overflowing_tokens=True,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        stride=stride,\n",
        "        padding='max_length' if padding else False\n",
        "    )\n",
        "\n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    ner_tags = []\n",
        "    relation_mask = []\n",
        "\n",
        "    for i, offset_mapping in enumerate(tokenized_inputs['offset_mapping']):\n",
        "        input_ids_segment = tokenized_inputs['input_ids'][i]\n",
        "        attention_mask_segment = tokenized_inputs['attention_mask'][i]\n",
        "\n",
        "        ner_tags_segment = [(0,) for i in range(max_length)]\n",
        "        rel_mask_segment = []\n",
        "        entity_map = {}\n",
        "        b_tag_indices = []\n",
        "\n",
        "        for entity_id, entity_type, (start_char, end_char) in zip(entities['id'], entities['type'], entities['offsets']):\n",
        "            entity_start_found = False\n",
        "            for token_idx, (token_start, token_end) in enumerate(offset_mapping):\n",
        "                if token_idx == 0 or token_idx == max_length - 1:\n",
        "                    continue\n",
        "                if token_start == start_char and offset_mapping[-2][1] >= end_char:\n",
        "                    ner_tags_segment[token_idx] += (tag_dict[f'B-{entity_type}'],)\n",
        "                    entity_map[entity_id] = [token_idx, token_idx]\n",
        "                    entity_start_found = True\n",
        "                elif entity_start_found and token_start < end_char:\n",
        "                    ner_tags_segment[token_idx] += (tag_dict[f'I-{entity_type}'],)\n",
        "                    entity_map[entity_id][1] = token_idx\n",
        "\n",
        "        rel_pairs = set()\n",
        "        for rel_type, arg1_id, arg2_id in zip(relations['type'], relations['arg1'], relations['arg2']):\n",
        "            if arg1_id in entity_map and arg2_id in entity_map:\n",
        "                rel_mask_segment.append([entity_map[arg1_id][0], entity_map[arg1_id][1], r_dict[rel_type], entity_map[arg2_id][0], entity_map[arg2_id][1]])\n",
        "                rel_pairs.add((entity_map[arg1_id][0], entity_map[arg1_id][1], entity_map[arg2_id][0], entity_map[arg2_id][1]))\n",
        "\n",
        "        for j in range(max_length):\n",
        "            ner_tags_segment[j] = tag_dict_map[tuple(sorted(list(set(ner_tags_segment[j]))))]\n",
        "            if ner_tags_segment[j] in {1, 2, 4, 7, 8, 12, 13, 15, 16, 17, 18}:\n",
        "                b_tag_indices.append(j)\n",
        "\n",
        "        relpairs = set(rel_pairs)\n",
        "        neg_samples = []\n",
        "        for i in rel_pairs:\n",
        "            for j in b_tag_indices:\n",
        "                tem = gen(i[0], j, ner_tags_segment, next_tags)\n",
        "                for ke in tem:\n",
        "                    if ke not in relpairs:\n",
        "                        relpairs.add(ke)\n",
        "                        neg_samples.append([ke[0], ke[1], 11, ke[2], ke[3]])\n",
        "                tem = gen(j, i[2], ner_tags_segment, next_tags)\n",
        "                for ke in tem:\n",
        "                    if ke not in relpairs:\n",
        "                        relpairs.add(ke)\n",
        "                        neg_samples.append([ke[0], ke[1], 11, ke[2], ke[3]])\n",
        "\n",
        "        neg_samples = random.sample(neg_samples, len(rel_pairs)) if len(rel_pairs) != 0 and len(rel_pairs) <= len(neg_samples) else random.sample(neg_samples, 15) if len(neg_samples) >= 15 else neg_samples\n",
        "\n",
        "        neg_samples_r = []\n",
        "        for i in b_tag_indices:\n",
        "            for j in b_tag_indices:\n",
        "                tem = gen(i, j, ner_tags_segment, next_tags)\n",
        "                for ke in tem:\n",
        "                    if ke not in relpairs:\n",
        "                        relpairs.add(ke)\n",
        "                        neg_samples_r.append([ke[0], ke[1], 11, ke[2], ke[3]])\n",
        "\n",
        "        neg_samples_r = random.sample(neg_samples_r, len(rel_pairs)) if len(rel_pairs) != 0 and len(rel_pairs) <= len(neg_samples_r) else random.sample(neg_samples_r, 15) if len(neg_samples_r) >= 15 else neg_samples_r\n",
        "        input_ids.append(input_ids_segment)\n",
        "        attention_mask.append(attention_mask_segment)\n",
        "        ner_tags.append(ner_tags_segment)\n",
        "        relation_mask.append(rel_mask_segment + neg_samples + neg_samples_r)\n",
        "\n",
        "    return input_ids, attention_mask, ner_tags, relation_mask\n",
        "\n"
      ],
      "metadata": {
        "id": "1_5B3Rxw5uB-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calculate_final_metrics(model, num_classes, threshold):\n",
        "    avg_precision, avg_recall, avg_f1_score = [], [], []\n",
        "    total_tp, total_fp, total_fn, total_tn = 0, 0, 0, 0\n",
        "    tpa,fpa,tna,fna,k=[],[],[],[],0\n",
        "    for i in range(num_classes):\n",
        "        tp, tn, fp, fn = model.metrics_dict[i].numpy()\n",
        "        if tp+fn < threshold:\n",
        "            continue\n",
        "\n",
        "        k += 1\n",
        "        tpa.append(tp)\n",
        "        fpa.append(fp)\n",
        "        tna.append(tn)\n",
        "        fna.append(fn)\n",
        "\n",
        "        total_tp += tp\n",
        "        total_fp += fp\n",
        "        total_fn += fn\n",
        "        total_tn += tn\n",
        "\n",
        "        precision = tp / (tp + fp + tf.keras.backend.epsilon())\n",
        "        recall = tp / (tp + fn + tf.keras.backend.epsilon())\n",
        "        f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
        "\n",
        "        avg_precision.append(precision)\n",
        "        avg_recall.append(recall)\n",
        "        avg_f1_score.append(f1)\n",
        "    accuracy = (total_tp ) / (total_tp + total_fp + tf.keras.backend.epsilon())\n",
        "    return {\n",
        "        \"average_precision\": tf.reduce_mean(avg_precision),\n",
        "        \"average_recall\": tf.reduce_mean(avg_recall),\n",
        "        \"average_f1_score\": tf.reduce_mean(avg_f1_score),\n",
        "        \"accuracy\": accuracy\n",
        "    }\n"
      ],
      "metadata": {
        "id": "S_UQSgm46eNP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "all_input_ids, all_attention_masks, all_ner_tags, all_relation_masks = [], [], [], []\n",
        "for row in ds['test']:\n",
        "    input_ids, attention_mask, ner_tags, relation_mask = prepare_data(row, tag_dict, r_dict, tag_dict_map, next_entity)\n",
        "    all_input_ids.extend(input_ids)\n",
        "    all_attention_masks.extend(attention_mask)\n",
        "    all_ner_tags.extend(ner_tags)\n",
        "    all_relation_masks.extend(relation_mask)\n",
        "\n",
        "prepared_data = {\n",
        "    'input_ids': np.array(all_input_ids),\n",
        "    'attention_mask': np.array(all_attention_masks),\n",
        "    'ner_tags': np.array(all_ner_tags),\n",
        "    're_mask': all_relation_masks\n",
        "}\n",
        "\n",
        "padded_relation_mask = pad_sequences(\n",
        "    [np.array(item, dtype=int) for item in prepared_data['re_mask']],\n",
        "    padding='post',\n",
        "    value=-1,\n",
        "    dtype='object'\n",
        ")\n",
        "\n",
        "padded_relation_mask = np.array(padded_relation_mask, dtype=int)\n",
        "\n",
        "inputs = {\n",
        "    'input_ids': prepared_data['input_ids'],\n",
        "    'attention_mask': prepared_data['attention_mask'],\n",
        "    're_mask': padded_relation_mask,\n",
        "    'ner_tags': prepared_data['ner_tags']\n",
        "}"
      ],
      "metadata": {
        "id": "KTK19d896GDw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_model.reset_metrics()\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': inputs['input_ids'],\n",
        "        'attention_mask': inputs['attention_mask']\n",
        "    },\n",
        "    inputs['ner_tags']\n",
        "))\n",
        "dataset = dataset.batch(batch_size)\n",
        "\n",
        "for batch_num, batch_data in enumerate(dataset.take(num_batches)):\n",
        "    ner_model.test_step(batch_data)\n"
      ],
      "metadata": {
        "id": "W9YPUPjm6QW4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re_model.reset_metrics()\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': inputs['input_ids'],\n",
        "        'attention_mask': inputs['attention_mask'],\n",
        "        're_mask': padded_relation_mask,\n",
        "        'ner_tags': inputs['ner_tags']\n",
        "    }\n",
        "))\n",
        "dataset = dataset.batch(batch_size)\n",
        "\n",
        "for batch_num, batch_data in enumerate(dataset.take(num_batches)):\n",
        "    re_model.test_step(batch_data)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JI6_QdtB6413"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('NER task metrics')\n",
        "ner_metrics = calculate_final_metrics(ner_model, 19, 10)\n",
        "for key in ner_metrics.keys():\n",
        "    print(f\"{key}: {ner_metrics[key]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FWpBYo92Fes",
        "outputId": "48ab54ea-898a-4bf4-888e-9bbad9bc7d7b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NER task metrics\n",
            "average_precision: 0.7057518311082904\n",
            "average_recall: 0.5608184309103597\n",
            "average_f1_score: 0.613044709264921\n",
            "accuracy: 0.9334979838707325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('RE task metrics')\n",
        "re_metrics = calculate_final_metrics(re_model, 12, 30)\n",
        "for key in re_metrics.keys():\n",
        "    print(f\"{key}: {re_metrics[key]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhja_3w02GYq",
        "outputId": "cf51033d-21aa-467d-c8e7-b1591bac8ba8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RE task metrics\n",
            "average_precision: 0.6087509847288989\n",
            "average_recall: 0.3772806177997443\n",
            "average_f1_score: 0.4455733635936242\n",
            "accuracy: 0.8706997941749827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install neo4j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7Du456PCzuU",
        "outputId": "08747211-6b82-456c-fd55-021128fe3fd3",
        "collapsed": true
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neo4j\n",
            "  Downloading neo4j-5.28.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from neo4j) (2025.2)\n",
            "Downloading neo4j-5.28.1-py3-none-any.whl (312 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/312.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m307.2/312.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.3/312.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: neo4j\n",
            "Successfully installed neo4j-5.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rel_dict = {\n",
        "    0: \"UNDEFINED\",\n",
        "    1: \"PART_OF\",\n",
        "    2: \"REGULATOR_OR_DIRECT_REGULATOR_OR_INDIRECT_REGULATOR_OF\",\n",
        "    3: \"ACTIVATOR_OR_UPREGULATOR_OR_INDIRECT_UPREGULATOR_OF\",\n",
        "    4: \"INHIBITOR_OR_DOWNREGULATOR_OR_INDIRECT_DOWNREGULATOR_OF\",\n",
        "    5: \"AGONIST_OR_AGONIST_ACTIVATOR_OR_AGONIST_INHIBITOR_OF\",\n",
        "    6: \"ANTAGONIST_OF\",\n",
        "    7: \"MODULATOR_OR_MODULATOR_ACTIVATOR_OR_MODULATOR_INHIBITOR_OF\",\n",
        "    8: \"COFACTOR_OF\",\n",
        "    9: \"SUBSTRATE_OR_PRODUCT_OR_SUBSTRATE_PRODUCT_OF\",\n",
        "    10: \"NO_RELATION\"\n",
        "}"
      ],
      "metadata": {
        "id": "jclKzgQ_WMhl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from neo4j import GraphDatabase\n",
        "\n",
        "#Enter Neo4j database credentials\n",
        "\n",
        "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
        "\n",
        "def delete_all_nodes_and_relationships(tx):\n",
        "    tx.run(\"MATCH (n) DETACH DELETE n\")\n",
        "\n",
        "with driver.session() as session:\n",
        "    session.execute_write(delete_all_nodes_and_relationships)\n",
        "\n",
        "def add_relations(text, offset_mapping, ner_segments, rel_segment, relations, tag_dict):\n",
        "    with driver.session() as session:\n",
        "        for i, rel in enumerate(rel_segment):\n",
        "            start1, end1, _, start2, end2 = rel\n",
        "            if relations[i] in {0, 10, 11}:\n",
        "                continue\n",
        "            relation_type = rel_dict[relations[i]]\n",
        "            ent1_offsets = offset_mapping[start1:end1 + 1]\n",
        "            ent2_offsets = offset_mapping[start2:end2 + 1]\n",
        "            ent1_text = text[ent1_offsets[0][0]:ent1_offsets[-1][1]]\n",
        "            ent2_text = text[ent2_offsets[0][0]:ent2_offsets[-1][1]]\n",
        "            ent1_type = tag_dict[ner_segments[i][0]]\n",
        "            ent2_type = tag_dict[ner_segments[i][1]]\n",
        "            session.execute_write(\n",
        "                create_or_update_relation,\n",
        "                ent1_text, ent1_type,\n",
        "                ent2_text, ent2_type,\n",
        "                relation_type\n",
        "            )\n",
        "\n",
        "def create_or_update_relation(tx, ent1_text, ent1_type, ent2_text, ent2_type, relation_type):\n",
        "    ent1_text = ent1_text.strip().capitalize()\n",
        "    ent2_text = ent2_text.strip().capitalize()\n",
        "    query = \"\"\"\n",
        "    MERGE (e1:%s {name: $ent1_text})\n",
        "    MERGE (e2:%s {name: $ent2_text})\n",
        "    MERGE (e1)-[:`%s`]->(e2)\n",
        "    \"\"\" % (ent1_type, ent2_type, relation_type)\n",
        "    tx.run(query, ent1_text=ent1_text, ent2_text=ent2_text)\n"
      ],
      "metadata": {
        "id": "WjGi4wpMwX-7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "b_i = {}\n",
        "for i in con.keys():\n",
        "    for j in con[i]:\n",
        "        b_i[j] = i-1\n",
        "\n",
        "\n",
        "def gen(i, j, ner_tags_segment, next_tags):\n",
        "    l, m = i + 1, j + 1\n",
        "    tem = []\n",
        "    types = []\n",
        "    while ner_tags_segment[l] in next_tags[ner_tags_segment[i]][0]:\n",
        "        l += 1\n",
        "    while ner_tags_segment[m] in next_tags[ner_tags_segment[j]][0]:\n",
        "        m += 1\n",
        "    if m - 1 != j and l - 1 != i:\n",
        "        tem.append((i, l - 1, j, m - 1))\n",
        "        types.append([b_i[next_tags[ner_tags_segment[i]][0][0]],b_i[next_tags[ner_tags_segment[j]][0][0]]])\n",
        "    l, m = i + 1, j + 1\n",
        "    while ner_tags_segment[l] in next_tags[ner_tags_segment[i]][0]:\n",
        "        l += 1\n",
        "    while ner_tags_segment[m] in next_tags[ner_tags_segment[j]][1]:\n",
        "        m += 1\n",
        "    if m - 1 != j and l - 1 != i:\n",
        "        tem.append((i, l - 1, j, m - 1))\n",
        "        types.append([b_i[next_tags[ner_tags_segment[i]][0][0]],b_i[next_tags[ner_tags_segment[j]][1][0]]])\n",
        "    l, m = i + 1, j + 1\n",
        "    while ner_tags_segment[l] in next_tags[ner_tags_segment[i]][1]:\n",
        "        l += 1\n",
        "    while ner_tags_segment[m] in next_tags[ner_tags_segment[j]][0]:\n",
        "        m += 1\n",
        "    if m - 1 != j and l - 1 != i:\n",
        "        tem.append((i, l - 1, j, m - 1))\n",
        "        types.append([b_i[next_tags[ner_tags_segment[i]][1][0]],b_i[next_tags[ner_tags_segment[j]][0][0]]])\n",
        "    l, m = i + 1, j + 1\n",
        "    while ner_tags_segment[l] in next_tags[ner_tags_segment[i]][1]:\n",
        "        l += 1\n",
        "    while ner_tags_segment[m] in next_tags[ner_tags_segment[j]][1]:\n",
        "        m += 1\n",
        "    if m - 1 != j and l - 1 != i:\n",
        "        tem.append((i, l - 1, j, m - 1))\n",
        "        types.append([b_i[next_tags[ner_tags_segment[i]][1][0]],b_i[next_tags[ner_tags_segment[j]][1][0]]])\n",
        "    return tem, types\n",
        "\n",
        "def infer(text, tag_dict, r_dict, tag_dict_map, next_tags, ner_model, re_model, max_length=128, stride=10, padding=True):\n",
        "\n",
        "\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        return_offsets_mapping=True,\n",
        "        return_overflowing_tokens=True,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        stride=stride,\n",
        "        padding='max_length' if padding else False\n",
        "    )\n",
        "\n",
        "    k=0\n",
        "    for (input_ids, attention_mask) in zip(tokenized_inputs['input_ids'], tokenized_inputs['attention_mask']):\n",
        "        ner_tags_segment = ner_model.test_step({'input_ids':np.array([input_ids]), 'attention_mask':np.array([attention_mask])}, inference = True)[0].numpy()\n",
        "        b_tag_indices = []\n",
        "        for i in range(max_length):\n",
        "            if attention_mask[i] == 0:\n",
        "                ner_tags_segment[i] = 0\n",
        "            if ner_tags_segment[i] in {1, 2, 4, 7, 8, 12, 13, 15, 16, 17, 18}:\n",
        "                b_tag_indices.append(i)\n",
        "        rel_segment = []\n",
        "        entity_types = []\n",
        "        for i in b_tag_indices:\n",
        "            for j in b_tag_indices:\n",
        "                tem, types = gen(i, j, ner_tags_segment, next_tags)\n",
        "                for (ke, t) in zip(tem, types):\n",
        "                    rel_segment.append([ke[0], ke[1], -1, ke[2], ke[3]])\n",
        "                    entity_types.append(t)\n",
        "        relations = re_model.test_step({'input_ids':np.array([input_ids]), 'attention_mask':np.array([attention_mask]), 're_mask':np.array([rel_segment]), 'ner_tags':np.array([ner_tags_segment])}, inference = True).numpy()\n",
        "        add_relations(text, tokenized_inputs['offset_mapping'][k], entity_types, rel_segment, relations, { 1:'GENE_Y', 3:'GENE_N', 5:'CHEMICAL'})\n",
        "        k += 1\n",
        "\n"
      ],
      "metadata": {
        "id": "ZkObu6zTji6V"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for row in ds['test']:\n",
        "    infer(row['text'], tag_dict, r_dict, tag_dict_map, next_entity, ner_model, re_model)"
      ],
      "metadata": {
        "id": "Sg8w6-vYMvMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver.close()"
      ],
      "metadata": {
        "id": "fmrKvIijEAia"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "myW4Jqsc5kAl"
      }
    }
  ]
}