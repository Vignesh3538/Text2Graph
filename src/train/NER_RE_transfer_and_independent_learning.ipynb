{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9u9X2EczuoA",
        "outputId": "8f99a31e-3fb9-4b93-a41d-5554de553dce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import TFBertModel\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "#tf.config.run_functions_eagerly(True)\n",
        "class CustomCRF(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_tags, seq_len, **kwargs):\n",
        "        super(CustomCRF, self).__init__(**kwargs)\n",
        "        self.num_tags = num_tags\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        xavier_initializer = tf.keras.initializers.GlorotUniform()\n",
        "        self.start_transitions = self.add_weight(\n",
        "            shape=(self.num_tags,), initializer=xavier_initializer, trainable=True\n",
        "        )\n",
        "        self.transition_matrix = self.add_weight(\n",
        "            shape=(self.num_tags, self.num_tags), initializer=xavier_initializer, trainable=True\n",
        "        )\n",
        "        self.end_transitions = self.add_weight(\n",
        "            shape=(self.num_tags,), initializer=xavier_initializer, trainable=True\n",
        "        )\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs, labels=None, training=None):\n",
        "        emissions, attention_mask = inputs\n",
        "        seq_len = tf.shape(emissions)[1]\n",
        "\n",
        "        if training:\n",
        "            return self._crf_loss(emissions, labels, attention_mask)\n",
        "        else:\n",
        "            return self.viterbi(emissions, attention_mask)\n",
        "\n",
        "    @tf.function\n",
        "    def _crf_loss(self, emissions, labels, attention_mask, seq_len):\n",
        "        log_likelihood = self._forward_algorithm(emissions, seq_len)\n",
        "        gold_score = self._score_sequence(emissions, labels, seq_len)\n",
        "        return tf.reduce_mean(log_likelihood - gold_score)\n",
        "\n",
        "    @tf.function\n",
        "    def _forward_algorithm(self, emissions, seq_len):\n",
        "        batch_size, num_tags = tf.shape(emissions)[0], self.num_tags\n",
        "\n",
        "        log_alpha = self.start_transitions + emissions[:, 0, :]\n",
        "\n",
        "        def step_fn(t, log_alpha):\n",
        "            log_alpha_expanded = tf.expand_dims(log_alpha, axis=2)\n",
        "            transition_scores = tf.expand_dims(self.transition_matrix, axis=0)\n",
        "            log_alpha_t = tf.reduce_logsumexp(log_alpha_expanded + transition_scores, axis=1)\n",
        "            new_log_alpha = log_alpha_t + emissions[:, t, :]\n",
        "            return t + 1, new_log_alpha\n",
        "\n",
        "        t = tf.constant(1)\n",
        "        _, log_alpha = tf.while_loop(\n",
        "            cond=lambda t, *_: t < seq_len,\n",
        "            body=step_fn,\n",
        "            loop_vars=[t, log_alpha]\n",
        "        )\n",
        "\n",
        "        log_alpha += self.end_transitions\n",
        "        return tf.reduce_logsumexp(log_alpha, axis=1)\n",
        "\n",
        "    @tf.function\n",
        "    def _score_sequence(self, emissions, labels, seq_len):\n",
        "        batch_size = tf.shape(labels)[0]\n",
        "\n",
        "        start_label_indices = labels[:, 0]\n",
        "        start_transition_scores = tf.gather(self.start_transitions, start_label_indices)\n",
        "\n",
        "        first_emission_scores = tf.gather_nd(\n",
        "            emissions,\n",
        "            indices=tf.stack(\n",
        "                [tf.range(batch_size, dtype=tf.int64), tf.zeros(batch_size, dtype=tf.int64), start_label_indices],\n",
        "                axis=1\n",
        "            )\n",
        "        )\n",
        "\n",
        "        score = start_transition_scores + first_emission_scores\n",
        "\n",
        "        for t in range(1, seq_len):\n",
        "            prev_labels = labels[:, t - 1]\n",
        "            curr_labels = labels[:, t]\n",
        "\n",
        "            indices = tf.stack([prev_labels, curr_labels], axis=1)\n",
        "            transition_scores = tf.gather_nd(self.transition_matrix, indices)\n",
        "            curr_emission_scores = tf.gather_nd(\n",
        "                emissions,\n",
        "                indices=tf.stack(\n",
        "                    [tf.range(batch_size, dtype=tf.int64), tf.cast(tf.fill([batch_size], t), tf.int64), curr_labels],\n",
        "                    axis=1\n",
        "                )\n",
        "            )\n",
        "\n",
        "            score += transition_scores + curr_emission_scores\n",
        "\n",
        "        end_label_indices = labels[:, seq_len - 1]\n",
        "        end_transition_scores = tf.gather(self.end_transitions, end_label_indices)\n",
        "\n",
        "        score += end_transition_scores\n",
        "\n",
        "        return score\n",
        "\n",
        "    @tf.function\n",
        "    def viterbi(self, emissions, attention_mask):\n",
        "        batch_size = tf.shape(emissions)[0]\n",
        "        seq_len = tf.shape(emissions)[1]\n",
        "        num_tags = tf.shape(emissions)[2]\n",
        "\n",
        "        dp = tf.TensorArray(dtype=tf.float32, size=seq_len, clear_after_read=False)\n",
        "        backpointer = tf.TensorArray(dtype=tf.int32, size=seq_len, clear_after_read=False)\n",
        "\n",
        "        first_step = self.start_transitions + emissions[:, 0, :]\n",
        "        dp = dp.write(0, first_step)\n",
        "\n",
        "        t = tf.constant(1)\n",
        "\n",
        "        def loop_body(t, dp, backpointer):\n",
        "            prev_scores = dp.read(t - 1)\n",
        "            scores = tf.expand_dims(prev_scores, axis=2) + self.transition_matrix\n",
        "            best_scores = tf.reduce_max(scores, axis=1)\n",
        "            best_paths = tf.argmax(scores, axis=1, output_type=tf.int32)\n",
        "            current_scores = emissions[:, t, :] + best_scores\n",
        "            dp = dp.write(t, current_scores)\n",
        "            backpointer = backpointer.write(t, best_paths)\n",
        "            return t + 1, dp, backpointer\n",
        "\n",
        "        _, dp, backpointer = tf.while_loop(\n",
        "            cond=lambda t, *_: t < seq_len,\n",
        "            body=loop_body,\n",
        "            loop_vars=(t, dp, backpointer)\n",
        "        )\n",
        "\n",
        "        last_step_scores = dp.read(seq_len - 1) + self.end_transitions\n",
        "        last_tag = tf.argmax(last_step_scores, axis=1, output_type=tf.int32)\n",
        "\n",
        "        def backtrack_fn(i):\n",
        "            best_path = tf.TensorArray(dtype=tf.int32, size=seq_len, clear_after_read=False)\n",
        "            best_path = best_path.write(seq_len - 1, last_tag[i])\n",
        "            t = seq_len - 2\n",
        "\n",
        "            def backtrack_body(t, best_path):\n",
        "                next_tag = best_path.read(t + 1)\n",
        "                best_tag = backpointer.read(t + 1)[i, next_tag]\n",
        "                best_path = best_path.write(t, best_tag)\n",
        "                return t - 1, best_path\n",
        "\n",
        "            _, best_path = tf.while_loop(\n",
        "                cond=lambda t, *_: t >= 0,\n",
        "                body=backtrack_body,\n",
        "                loop_vars=(t, best_path)\n",
        "            )\n",
        "\n",
        "            return best_path.stack()\n",
        "\n",
        "        best_paths = tf.map_fn(backtrack_fn, tf.range(batch_size), fn_output_signature=tf.int32)\n",
        "        return best_paths\n",
        "\n",
        "\n",
        "class NERModel(tf.keras.Model):\n",
        "    def __init__(self, num_tags, hidden_dim, seq_len, **kwargs):\n",
        "        super(NERModel, self).__init__(**kwargs)\n",
        "        self.bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        self.lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hidden_dim, return_sequences=True))\n",
        "        self.dense = tf.keras.layers.Dense(num_tags)\n",
        "        self.num_tags = num_tags\n",
        "        self.metrics_dict = {i: tf.Variable([0, 0, 0, 0], dtype=tf.int32) for i in range(num_tags)}\n",
        "        self.crf = CustomCRF(num_tags, seq_len)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.crf.build(input_shape)\n",
        "    @tf.function\n",
        "    def call(self, inputs, labels=None, training=None):\n",
        "        bert_output = self.bert(inputs['input_ids'], attention_mask=inputs['attention_mask'])['last_hidden_state']\n",
        "        lstm_output = self.lstm(tf.nn.l2_normalize(bert_output, axis=-1), training=training)\n",
        "        logits = self.dense(lstm_output)\n",
        "\n",
        "        if training:\n",
        "            return logits\n",
        "        else:\n",
        "            return self.crf((logits, inputs['attention_mask']), training=False)\n",
        "    @tf.function\n",
        "    def train_step(self, data):\n",
        "        inputs, labels = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            emissions = self(inputs, training=True)  # Forward pass\n",
        "            loss = self.crf._crf_loss(emissions, labels, inputs['attention_mask'],tf.shape(emissions)[1])\n",
        "\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        return {\"loss\": loss}\n",
        "    def reset_metrics(self):\n",
        "        for key in self.metrics_dict:\n",
        "            self.metrics_dict[key].assign([0, 0, 0, 0])\n",
        "\n",
        "    def test_step(self, data, validation=False):\n",
        "        inputs, labels = data\n",
        "        emissions = self(inputs, training=validation)\n",
        "        if validation:\n",
        "            loss = self.crf._crf_loss(emissions, labels, inputs['attention_mask'],tf.shape(emissions)[1])\n",
        "            return {'loss':loss}\n",
        "        predictions = tf.reshape(emissions, [-1])\n",
        "        true_labels = tf.reshape(labels, [-1])\n",
        "\n",
        "        for i in range(self.num_tags):\n",
        "            true_positives = tf.reduce_sum(tf.cast((predictions == i) & (true_labels == i), tf.int32))\n",
        "            false_positives = tf.reduce_sum(tf.cast((predictions == i) & (true_labels != i), tf.int32))\n",
        "            false_negatives = tf.reduce_sum(tf.cast((predictions != i) & (true_labels == i), tf.int32))\n",
        "            true_negatives = tf.reduce_sum(tf.cast((predictions != i) & (true_labels != i), tf.int32))\n",
        "\n",
        "            self.metrics_dict[i].assign_add([true_positives, true_negatives, false_positives, false_negatives])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ny5CN1ysz47k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class RelationExtractionModel(tf.keras.Model):\n",
        "    def __init__(self, ner_model, num_relations, hidden_dim, dropout_rate=0.2):\n",
        "        super(RelationExtractionModel, self).__init__()\n",
        "        self.bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "        self.layer_norm_ip = tf.keras.layers.LayerNormalization()\n",
        "        self.layer_norm_ip.build((None, None, self.bert.config.hidden_size))\n",
        "        self.bilstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hidden_dim, return_sequences=True))\n",
        "        self.bilstm.build((None, None, ner_model.bert.config.hidden_size))\n",
        "        self.bilstm.set_weights(ner_model.lstm.get_weights())  # comment this line for independent learning\n",
        "        self.layer_norm_op = tf.keras.layers.LayerNormalization()\n",
        "        self.layer_norm_op.build((None, None, 2 * hidden_dim))\n",
        "        self.dense = tf.keras.layers.Dense(num_relations, activation='softmax')\n",
        "        self.dense.build((None, 4 * hidden_dim))\n",
        "        self.num_relations = num_relations\n",
        "        self.metrics_dict = {i: tf.Variable([0, 0, 0, 0], dtype=tf.int32) for i in range(num_relations)}\n",
        "    @tf.function\n",
        "    def call(self, inputs, training=False):\n",
        "        input_ids = inputs['input_ids']\n",
        "        attention_mask = inputs['attention_mask']\n",
        "        ner_tags = inputs['ner_tags']\n",
        "        re_mask = inputs['re_mask']\n",
        "\n",
        "        bert_output = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        token_embeddings = bert_output.last_hidden_state\n",
        "        lstm_output = self.layer_norm_op(self.bilstm(self.layer_norm_ip(token_embeddings), training=training))\n",
        "        combined_embeddings = self.extract_entity_pairs(lstm_output, ner_tags, re_mask)\n",
        "        combined_embeddings_stacked = tf.stack(combined_embeddings)\n",
        "        return self.dense(combined_embeddings_stacked)\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def extract_entity_pairs(self, lstm_output, ner_tags, re_mask):\n",
        "        combined_embeddings = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "        batch_size = tf.shape(ner_tags)[0]\n",
        "\n",
        "        def process_batch(i, embeddings_list):\n",
        "            re_pairs = re_mask[i]\n",
        "            valid_pairs = tf.boolean_mask(re_pairs, tf.not_equal(re_pairs[:, 0], -1))\n",
        "\n",
        "            def process_pair(j, embeddings_list):\n",
        "                e1_sidx, e1_eidx, relation_type, e2_sidx, e2_eidx = tf.unstack(valid_pairs[j])\n",
        "                e1_emb = self.pool_entity(lstm_output[i], ner_tags[i], e1_sidx, e1_eidx)\n",
        "                e2_emb = self.pool_entity(lstm_output[i], ner_tags[i], e2_sidx, e2_eidx)\n",
        "                combined = tf.concat([e1_emb, e2_emb], axis=-1)\n",
        "                return j + 1, embeddings_list.write(embeddings_list.size(), combined)\n",
        "\n",
        "            _, embeddings_list = tf.while_loop(\n",
        "                lambda j, _: j < tf.shape(valid_pairs)[0],\n",
        "                process_pair,\n",
        "                loop_vars=[0, embeddings_list]\n",
        "            )\n",
        "            return i + 1, embeddings_list\n",
        "\n",
        "        _, final_embeddings = tf.while_loop(\n",
        "            lambda i, _: i < batch_size,\n",
        "            process_batch,\n",
        "            loop_vars=[0, combined_embeddings]\n",
        "        )\n",
        "\n",
        "        return final_embeddings.stack()\n",
        "    @tf.function\n",
        "    def pool_entity(self, lstm_output, ner_tags, start_idx, end_idx):\n",
        "        entity_span = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "\n",
        "        def condition(idx, _):\n",
        "            return idx <= end_idx\n",
        "\n",
        "        def body(idx, entity_span):\n",
        "            entity_span = entity_span.write(entity_span.size(), lstm_output[idx])\n",
        "            return idx + 1, entity_span\n",
        "\n",
        "        _, collected_span = tf.while_loop(\n",
        "            condition, body, loop_vars=[start_idx, entity_span]\n",
        "        )\n",
        "\n",
        "        return tf.reduce_mean(collected_span.stack(), axis=0)\n",
        "\n",
        "    @tf.function\n",
        "    def extract_relation_labels(self, re_mask):\n",
        "        reduced_re_mask = re_mask[:, :, 2]\n",
        "        valid_mask = tf.not_equal(reduced_re_mask, -1)\n",
        "        valid_relation_labels = tf.boolean_mask(reduced_re_mask, valid_mask)\n",
        "        return valid_relation_labels\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, inputs):\n",
        "        filtered_relation_labels = self.extract_relation_labels(inputs['re_mask'])\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = self(inputs, training=True)\n",
        "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(\n",
        "                filtered_relation_labels, logits\n",
        "            ))\n",
        "\n",
        "        gradients = tape.gradient(loss, self.trainable_variables)\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        return {\"loss\": loss}\n",
        "    def reset_metrics(self):\n",
        "        for key in self.metrics_dict:\n",
        "            self.metrics_dict[key].assign([0, 0, 0, 0])\n",
        "\n",
        "    def test_step(self, inputs, validation = False):\n",
        "        true_labels = self.extract_relation_labels(inputs['re_mask'])\n",
        "        if validation :\n",
        "            logits = self(inputs, training=True)\n",
        "            loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(\n",
        "                true_labels, logits\n",
        "            ))\n",
        "            return {'loss':loss}\n",
        "        logits = self(inputs, training=False)\n",
        "        predictions = tf.argmax(logits, axis=-1)\n",
        "        for i in range(self.num_relations):\n",
        "            tp = tf.reduce_sum(tf.cast((predictions == i) & (true_labels == i), tf.int32))\n",
        "            fp = tf.reduce_sum(tf.cast((predictions == i) & (true_labels != i), tf.int32))\n",
        "            fn = tf.reduce_sum(tf.cast((predictions != i) & (true_labels == i), tf.int32))\n",
        "            tn = tf.reduce_sum(tf.cast((predictions != i) & (true_labels != i), tf.int32))\n",
        "\n",
        "            self.metrics_dict[i].assign_add([tp, tn, fp, fn])\n",
        "\n"
      ],
      "metadata": {
        "id": "oXLkciOzZMxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import random\n",
        "from transformers import AutoTokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "ds = load_dataset(\"bigbio/chemprot\", \"chemprot_full_source\")\n",
        "\n",
        "def gen(i, j, ner_tags_segment, next_tags):\n",
        "    l, m = i + 1, j + 1\n",
        "    tem = []\n",
        "    while ner_tags_segment[l] in next_tags[ner_tags_segment[i]][0]:\n",
        "        l += 1\n",
        "    while ner_tags_segment[m] in next_tags[ner_tags_segment[j]][0]:\n",
        "        m += 1\n",
        "    if m - 1 != j and l - 1 != i:\n",
        "        tem.append((i, l - 1, j, m - 1))\n",
        "    l, m = i + 1, j + 1\n",
        "    while ner_tags_segment[l] in next_tags[ner_tags_segment[i]][0]:\n",
        "        l += 1\n",
        "    while ner_tags_segment[m] in next_tags[ner_tags_segment[j]][1]:\n",
        "        m += 1\n",
        "    if m - 1 != j and l - 1 != i:\n",
        "        tem.append((i, l - 1, j, m - 1))\n",
        "    l, m = i + 1, j + 1\n",
        "    while ner_tags_segment[l] in next_tags[ner_tags_segment[i]][1]:\n",
        "        l += 1\n",
        "    while ner_tags_segment[m] in next_tags[ner_tags_segment[j]][0]:\n",
        "        m += 1\n",
        "    if m - 1 != j and l - 1 != i:\n",
        "        tem.append((i, l - 1, j, m - 1))\n",
        "    l, m = i + 1, j + 1\n",
        "    while ner_tags_segment[l] in next_tags[ner_tags_segment[i]][1]:\n",
        "        l += 1\n",
        "    while ner_tags_segment[m] in next_tags[ner_tags_segment[j]][1]:\n",
        "        m += 1\n",
        "    if m - 1 != j and l - 1 != i:\n",
        "        tem.append((i, l - 1, j, m - 1))\n",
        "    return tem\n",
        "\n",
        "def prepare_data(row, tag_dict, r_dict, tag_dict_map, next_tags, max_length=128, stride=10, padding=True):\n",
        "    text = row['text']\n",
        "    entities = row['entities']\n",
        "    relations = row['relations']\n",
        "\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        return_offsets_mapping=True,\n",
        "        return_overflowing_tokens=True,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        stride=stride,\n",
        "        padding='max_length' if padding else False\n",
        "    )\n",
        "\n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    ner_tags = []\n",
        "    relation_mask = []\n",
        "\n",
        "    for i, offset_mapping in enumerate(tokenized_inputs['offset_mapping']):\n",
        "        input_ids_segment = tokenized_inputs['input_ids'][i]\n",
        "        attention_mask_segment = tokenized_inputs['attention_mask'][i]\n",
        "\n",
        "        ner_tags_segment = [(0,) for i in range(max_length)]\n",
        "        rel_mask_segment = []\n",
        "        entity_map = {}\n",
        "        b_tag_indices = []\n",
        "\n",
        "        for entity_id, entity_type, (start_char, end_char) in zip(entities['id'], entities['type'], entities['offsets']):\n",
        "            entity_start_found = False\n",
        "            for token_idx, (token_start, token_end) in enumerate(offset_mapping):\n",
        "                if token_idx == 0 or token_idx == max_length - 1:\n",
        "                    continue\n",
        "                if token_start == start_char and offset_mapping[-2][1] >= end_char:\n",
        "                    ner_tags_segment[token_idx] += (tag_dict[f'B-{entity_type}'],)\n",
        "                    entity_map[entity_id] = [token_idx, token_idx]\n",
        "                    entity_start_found = True\n",
        "                elif entity_start_found and token_start < end_char:\n",
        "                    ner_tags_segment[token_idx] += (tag_dict[f'I-{entity_type}'],)\n",
        "                    entity_map[entity_id][1] = token_idx\n",
        "\n",
        "        rel_pairs = set()\n",
        "        for rel_type, arg1_id, arg2_id in zip(relations['type'], relations['arg1'], relations['arg2']):\n",
        "            if arg1_id in entity_map and arg2_id in entity_map:\n",
        "                rel_mask_segment.append([entity_map[arg1_id][0], entity_map[arg1_id][1], r_dict[rel_type], entity_map[arg2_id][0], entity_map[arg2_id][1]])\n",
        "                rel_pairs.add((entity_map[arg1_id][0], entity_map[arg1_id][1], entity_map[arg2_id][0], entity_map[arg2_id][1]))\n",
        "\n",
        "        for j in range(max_length):\n",
        "            ner_tags_segment[j] = tag_dict_map[tuple(sorted(list(set(ner_tags_segment[j]))))]\n",
        "            if ner_tags_segment[j] in {1, 2, 4, 7, 8, 12, 13, 15, 16, 17, 18}:\n",
        "                b_tag_indices.append(j)\n",
        "\n",
        "        relpairs = set(rel_pairs)\n",
        "        neg_samples = []\n",
        "        for i in rel_pairs:\n",
        "            for j in b_tag_indices:\n",
        "                tem = gen(i[0], j, ner_tags_segment, next_tags)\n",
        "                for ke in tem:\n",
        "                    if ke not in relpairs:\n",
        "                        relpairs.add(ke)\n",
        "                        neg_samples.append([ke[0], ke[1], 11, ke[2], ke[3]])\n",
        "                tem = gen(j, i[2], ner_tags_segment, next_tags)\n",
        "                for ke in tem:\n",
        "                    if ke not in relpairs:\n",
        "                        relpairs.add(ke)\n",
        "                        neg_samples.append([ke[0], ke[1], 11, ke[2], ke[3]])\n",
        "\n",
        "        neg_samples = random.sample(neg_samples, len(rel_pairs)) if len(rel_pairs) != 0 and len(rel_pairs) <= len(neg_samples) else random.sample(neg_samples, 15) if len(neg_samples) >= 15 else neg_samples\n",
        "\n",
        "        neg_samples_r = []\n",
        "        for i in b_tag_indices:\n",
        "            for j in b_tag_indices:\n",
        "                tem = gen(i, j, ner_tags_segment, next_tags)\n",
        "                for ke in tem:\n",
        "                    if ke not in relpairs:\n",
        "                        relpairs.add(ke)\n",
        "                        neg_samples_r.append([ke[0], ke[1], 11, ke[2], ke[3]])\n",
        "\n",
        "        neg_samples_r = random.sample(neg_samples_r, len(rel_pairs)) if len(rel_pairs) != 0 and len(rel_pairs) <= len(neg_samples_r) else random.sample(neg_samples_r, 15) if len(neg_samples_r) >= 15 else neg_samples_r\n",
        "        input_ids.append(input_ids_segment)\n",
        "        attention_mask.append(attention_mask_segment)\n",
        "        ner_tags.append(ner_tags_segment)\n",
        "        relation_mask.append(rel_mask_segment + neg_samples + neg_samples_r)\n",
        "\n",
        "    return input_ids, attention_mask, ner_tags, relation_mask\n",
        "\n"
      ],
      "metadata": {
        "id": "RcHUZeB4O9Hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_dict = {\n",
        "    'B-GENE-Y': 1, 'I-GENE-Y': 2,\n",
        "    'B-GENE-N': 3, 'I-GENE-N': 4,\n",
        "    'B-CHEMICAL': 5, 'I-CHEMICAL': 6\n",
        "}\n",
        "\n",
        "tag_dict_map = {\n",
        "    (0,): 0, (0, 1): 1, (0, 2, 3, 5): 2,\n",
        "    (0, 4): 3, (0, 1, 6): 4, (0, 2, 6): 5, (0, 2): 6, (0, 3, 6): 7,\n",
        "    (0, 5): 8, (0, 4, 6): 9, (0, 2, 4, 6): 10, (0, 2, 4): 11,\n",
        "    (0, 5, 6): 12, (0, 3): 13, (0, 6): 14, (0, 1, 5): 15, (0, 2, 5): 16, (0, 3, 5): 17, (0, 4, 5): 18\n",
        "}\n",
        "\n",
        "con = {2:[2,5,6,10,11,16], 4:[3,9,10,11,18], 6:[4,5,7,9,10,12,14]}\n",
        "\n",
        "next_entity = {0:[[0],[]], 1:[con[2],[]], 2:[con[4],con[6]], 3:[[3],[]], 4:[con[2],[]], 5:[[5],[]], 6:[[6],[]], 7:[con[4],[]], 8:[con[6],[]], 9:[[9],[]] ,10:[[10],[]], 11:[[11],[]], 12:[con[6],[]], 13:[con[4],[]], 14:[[14],[]], 15:[con[2],con[6]], 16:[con[6],[]], 17:[con[4],con[6]], 18:[con[6],[]]}\n",
        "\n",
        "r_dict = {\n",
        "    'CPR:0': 0, 'CPR:1': 1, 'CPR:2': 2, 'CPR:3': 3,\n",
        "    'CPR:4': 4, 'CPR:5': 5, 'CPR:6': 6, 'CPR:7': 7,\n",
        "    'CPR:8': 8, 'CPR:9': 9, 'CPR:10': 10\n",
        "}\n",
        "\n",
        "all_input_ids, all_attention_masks, all_ner_tags, all_relation_masks = [], [], [], []\n",
        "for row in ds['train']:\n",
        "    input_ids, attention_mask, ner_tags, relation_mask = prepare_data(row, tag_dict, r_dict, tag_dict_map, next_entity)\n",
        "    all_input_ids.extend(input_ids)\n",
        "    all_attention_masks.extend(attention_mask)\n",
        "    all_ner_tags.extend(ner_tags)\n",
        "    all_relation_masks.extend(relation_mask)\n",
        "\n",
        "prepared_data = {\n",
        "    'input_ids': np.array(all_input_ids),\n",
        "    'attention_mask': np.array(all_attention_masks),\n",
        "    'ner_tags': np.array(all_ner_tags),\n",
        "    're_mask': all_relation_masks\n",
        "}\n",
        "\n",
        "hidden_dim = 64\n",
        "num_tags = 19\n",
        "seq_len = 128\n",
        "num_relations = 12\n",
        "\n",
        "padded_relation_mask = pad_sequences(\n",
        "    [np.array(item, dtype=int) for item in prepared_data['re_mask']],\n",
        "    padding='post',\n",
        "    value=-1,\n",
        "    dtype='object'\n",
        ")\n",
        "\n",
        "padded_relation_mask = np.array(padded_relation_mask, dtype=int)\n",
        "\n",
        "inputs = {\n",
        "    'input_ids': prepared_data['input_ids'],\n",
        "    'attention_mask': prepared_data['attention_mask'],\n",
        "    're_mask': padded_relation_mask,\n",
        "    'ner_tags': prepared_data['ner_tags']\n",
        "}\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': inputs['input_ids'],\n",
        "        'attention_mask': inputs['attention_mask']\n",
        "    },\n",
        "    inputs['ner_tags']\n",
        "))\n",
        "\n",
        "\n",
        "vall_input_ids, vall_attention_masks, vall_ner_tags, vall_relation_masks = [], [], [], []\n",
        "for row in ds['validation']:\n",
        "    input_ids, attention_mask, ner_tags, relation_mask = prepare_data(row, tag_dict, r_dict, tag_dict_map, next_entity)\n",
        "    vall_input_ids.extend(input_ids)\n",
        "    vall_attention_masks.extend(attention_mask)\n",
        "    vall_ner_tags.extend(ner_tags)\n",
        "    vall_relation_masks.extend(relation_mask)\n",
        "\n",
        "vprepared_data = {\n",
        "    'input_ids': np.array(vall_input_ids),\n",
        "    'attention_mask': np.array(vall_attention_masks),\n",
        "    'ner_tags': np.array(vall_ner_tags),\n",
        "    're_mask': vall_relation_masks\n",
        "}\n",
        "\n",
        "\n",
        "vpadded_relation_mask = pad_sequences(\n",
        "    [np.array(item, dtype=int) for item in vprepared_data['re_mask']],\n",
        "    padding='post',\n",
        "    value=-1,\n",
        "    dtype='object'\n",
        ")\n",
        "\n",
        "vpadded_relation_mask = np.array(vpadded_relation_mask, dtype=int)\n",
        "\n",
        "vinputs = {\n",
        "    'input_ids': vprepared_data['input_ids'],\n",
        "    'attention_mask': vprepared_data['attention_mask'],\n",
        "    're_mask': vpadded_relation_mask,\n",
        "    'ner_tags': vprepared_data['ner_tags']\n",
        "}\n",
        "\n",
        "vdataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': vinputs['input_ids'],\n",
        "        'attention_mask': vinputs['attention_mask']\n",
        "    },\n",
        "    vinputs['ner_tags']\n",
        "))\n",
        "\n",
        "batch_size = 30\n",
        "dataset = dataset.batch(batch_size)\n",
        "vdataset = vdataset.batch(batch_size)\n",
        "num_epochs = 30\n",
        "num_batches = 131\n"
      ],
      "metadata": {
        "id": "ZLmW7qzM6w2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ner_model = NERModel(num_tags, 64, seq_len)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "ner_model.compile(optimizer=optimizer)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    train_loss = 0\n",
        "    val_loss = 0\n",
        "    t_b, v_b = 0, 0\n",
        "\n",
        "    for batch_num, batch_data in enumerate(dataset.take(num_batches)):\n",
        "        train_loss += ner_model.train_step(batch_data)['loss']\n",
        "        t_b += 1\n",
        "    for batch_num, batch_data in enumerate(vdataset.take(num_batches)):\n",
        "        val_loss += ner_model.test_step(batch_data, validation = True)['loss']\n",
        "\n",
        "        v_b += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"End of Epoch {epoch + 1}, train loss: {train_loss / t_b} validation loss: {val_loss / v_b}\")"
      ],
      "metadata": {
        "id": "p-JVgPWSO5Hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': inputs['input_ids'],\n",
        "        'attention_mask': inputs['attention_mask'],\n",
        "        're_mask': padded_relation_mask,\n",
        "        'ner_tags': inputs['ner_tags']\n",
        "    }\n",
        "))\n",
        "dataset = dataset.batch(batch_size)\n",
        "\n",
        "vdataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': vinputs['input_ids'],\n",
        "        'attention_mask': vinputs['attention_mask'],\n",
        "        're_mask': vpadded_relation_mask,\n",
        "        'ner_tags': vinputs['ner_tags']\n",
        "    }\n",
        "))\n",
        "vdataset = vdataset.batch(batch_size)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "re_model = RelationExtractionModel(ner_model, num_relations, hidden_dim)\n",
        "re_model.compile(optimizer=optimizer)\n",
        "num_epochs=20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    train_loss = 0\n",
        "    val_loss = 0\n",
        "    t_b, v_b = 0, 0\n",
        "    for batch_num, batch_data in enumerate(dataset.take(num_batches)):\n",
        "        train_loss += re_model.train_step(batch_data)['loss']\n",
        "        t_b += 1\n",
        "    for batch_num, batch_data in enumerate(vdataset.take(num_batches)):\n",
        "        val_loss += re_model.test_step(batch_data, validation = True)['loss']\n",
        "        v_b += 1\n",
        "\n",
        "    print(f\"End of Epoch {epoch + 1}, train loss: {train_loss / t_b} validation loss: {val_loss / v_b}\")"
      ],
      "metadata": {
        "id": "slfY0QoW7V8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "all_input_ids, all_attention_masks, all_ner_tags, all_relation_masks = [], [], [], []\n",
        "for row in ds['test']:\n",
        "    input_ids, attention_mask, ner_tags, relation_mask = prepare_data(row, tag_dict, r_dict, tag_dict_map, next_entity)\n",
        "    all_input_ids.extend(input_ids)\n",
        "    all_attention_masks.extend(attention_mask)\n",
        "    all_ner_tags.extend(ner_tags)\n",
        "    all_relation_masks.extend(relation_mask)\n",
        "\n",
        "prepared_data = {\n",
        "    'input_ids': np.array(all_input_ids),\n",
        "    'attention_mask': np.array(all_attention_masks),\n",
        "    'ner_tags': np.array(all_ner_tags),\n",
        "    're_mask': all_relation_masks\n",
        "}\n",
        "\n",
        "padded_relation_mask = pad_sequences(\n",
        "    [np.array(item, dtype=int) for item in prepared_data['re_mask']],\n",
        "    padding='post',\n",
        "    value=-1,\n",
        "    dtype='object'\n",
        ")\n",
        "\n",
        "padded_relation_mask = np.array(padded_relation_mask, dtype=int)\n",
        "\n",
        "inputs = {\n",
        "    'input_ids': prepared_data['input_ids'],\n",
        "    'attention_mask': prepared_data['attention_mask'],\n",
        "    're_mask': padded_relation_mask,\n",
        "    'ner_tags': prepared_data['ner_tags']\n",
        "}"
      ],
      "metadata": {
        "id": "JJhEDXN38cek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_model.reset_metrics()\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': inputs['input_ids'],\n",
        "        'attention_mask': inputs['attention_mask']\n",
        "    },\n",
        "    inputs['ner_tags']\n",
        "))\n",
        "dataset = dataset.batch(batch_size)\n",
        "\n",
        "for batch_num, batch_data in enumerate(dataset.take(num_batches)):\n",
        "    ner_model.test_step(batch_data)\n"
      ],
      "metadata": {
        "id": "If4UHu5d9iFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re_model.reset_metrics()\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': inputs['input_ids'],\n",
        "        'attention_mask': inputs['attention_mask'],\n",
        "        're_mask': padded_relation_mask,\n",
        "        'ner_tags': inputs['ner_tags']\n",
        "    }\n",
        "))\n",
        "dataset = dataset.batch(batch_size)\n",
        "\n",
        "for batch_num, batch_data in enumerate(dataset.take(num_batches)):\n",
        "    re_model.test_step(batch_data)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Mt7f_Yws8gkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = '/content/re_model.keras'\n",
        "re_model.save(model_save_path)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(model_save_path)\n"
      ],
      "metadata": {
        "id": "dWENuuv-brCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = '/content/ner_model.keras'\n",
        "ner_model.save(model_save_path)\n",
        "\n",
        "files.download(model_save_path)\n"
      ],
      "metadata": {
        "id": "VtTc8i66Plz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def calculate_final_metrics(model, num_classes, threshold):\n",
        "    avg_precision, avg_recall, avg_f1_score = [], [], []\n",
        "    total_tp, total_fp, total_fn, total_tn = 0, 0, 0, 0\n",
        "    tpa,fpa,tna,fna,k=[],[],[],[],0\n",
        "    for i in range(num_classes):\n",
        "        tp, tn, fp, fn = model.metrics_dict[i].numpy()\n",
        "        if tp+fn < threshold:\n",
        "            continue\n",
        "\n",
        "        k += 1\n",
        "        tpa.append(tp)\n",
        "        fpa.append(fp)\n",
        "        tna.append(tn)\n",
        "        fna.append(fn)\n",
        "\n",
        "        total_tp += tp\n",
        "        total_fp += fp\n",
        "        total_fn += fn\n",
        "        total_tn += tn\n",
        "\n",
        "        precision = tp / (tp + fp + tf.keras.backend.epsilon())\n",
        "        recall = tp / (tp + fn + tf.keras.backend.epsilon())\n",
        "        f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
        "\n",
        "        avg_precision.append(precision)\n",
        "        avg_recall.append(recall)\n",
        "        avg_f1_score.append(f1)\n",
        "    print(avg_precision)\n",
        "    print(avg_recall)\n",
        "    print(avg_f1_score)\n",
        "    accuracy = (total_tp ) / (total_tp + total_fp + tf.keras.backend.epsilon())\n",
        "    return {\n",
        "        \"average_precision\": tf.reduce_mean(avg_precision),\n",
        "        \"average_recall\": tf.reduce_mean(avg_recall),\n",
        "        \"average_f1_score\": tf.reduce_mean(avg_f1_score),\n",
        "        \"accuracy\": accuracy\n",
        "    }\n"
      ],
      "metadata": {
        "id": "rNzlqh4sC9yK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('NER task metrics')\n",
        "ner_metrics = calculate_final_metrics(ner_model, 19, 10)\n",
        "for key in ner_metrics.keys():\n",
        "    print(f\"{key}: {ner_metrics[key]}\")"
      ],
      "metadata": {
        "id": "U5uGD9KHdrCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('RE task metrics')\n",
        "re_metrics = calculate_final_metrics(re_model, 12, 30)\n",
        "for key in re_metrics.keys():\n",
        "    print(f\"{key}: {re_metrics[key]}\")"
      ],
      "metadata": {
        "id": "mPQW94vWzv0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lqb4r9hy2BPD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}