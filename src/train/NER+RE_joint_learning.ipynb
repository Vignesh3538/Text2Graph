{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lP6DU2k_QTx8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "#tf.config.run_functions_eagerly(True)\n",
        "class CustomCRF(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_tags, seq_len, **kwargs):\n",
        "        super(CustomCRF, self).__init__(**kwargs)\n",
        "        self.num_tags = num_tags\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        xavier_initializer = tf.keras.initializers.GlorotUniform()\n",
        "        self.start_transitions = self.add_weight(\n",
        "            shape=(self.num_tags,), initializer=xavier_initializer, trainable=True\n",
        "        )\n",
        "        self.transition_matrix = self.add_weight(\n",
        "            shape=(self.num_tags, self.num_tags), initializer=xavier_initializer, trainable=True\n",
        "        )\n",
        "        self.end_transitions = self.add_weight(\n",
        "            shape=(self.num_tags,), initializer=xavier_initializer, trainable=True\n",
        "        )\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs, labels=None, training=None):\n",
        "        emissions, attention_mask = inputs\n",
        "        seq_len = tf.shape(emissions)[1]\n",
        "\n",
        "        if training:\n",
        "            return self._crf_loss(emissions, labels, attention_mask)\n",
        "        else:\n",
        "            return self.viterbi(emissions, attention_mask)\n",
        "\n",
        "    @tf.function\n",
        "    def _crf_loss(self, emissions, labels, attention_mask, seq_len):\n",
        "        log_likelihood = self._forward_algorithm(emissions, seq_len)\n",
        "        gold_score = self._score_sequence(emissions, labels, seq_len)\n",
        "        return tf.reduce_mean(log_likelihood - gold_score)\n",
        "\n",
        "    @tf.function\n",
        "    def _forward_algorithm(self, emissions, seq_len):\n",
        "        batch_size, num_tags = tf.shape(emissions)[0], self.num_tags\n",
        "\n",
        "        log_alpha = self.start_transitions + emissions[:, 0, :]\n",
        "\n",
        "        def step_fn(t, log_alpha):\n",
        "            log_alpha_expanded = tf.expand_dims(log_alpha, axis=2)\n",
        "            transition_scores = tf.expand_dims(self.transition_matrix, axis=0)\n",
        "            log_alpha_t = tf.reduce_logsumexp(log_alpha_expanded + transition_scores, axis=1)\n",
        "            new_log_alpha = log_alpha_t + emissions[:, t, :]\n",
        "            return t + 1, new_log_alpha\n",
        "\n",
        "        t = tf.constant(1)\n",
        "        _, log_alpha = tf.while_loop(\n",
        "            cond=lambda t, *_: t < seq_len,\n",
        "            body=step_fn,\n",
        "            loop_vars=[t, log_alpha]\n",
        "        )\n",
        "\n",
        "        log_alpha += self.end_transitions\n",
        "        return tf.reduce_logsumexp(log_alpha, axis=1)\n",
        "\n",
        "    @tf.function\n",
        "    def _score_sequence(self, emissions, labels, seq_len):\n",
        "        batch_size = tf.shape(labels)[0]\n",
        "\n",
        "        start_label_indices = labels[:, 0]\n",
        "        start_transition_scores = tf.gather(self.start_transitions, start_label_indices)\n",
        "\n",
        "        first_emission_scores = tf.gather_nd(\n",
        "            emissions,\n",
        "            indices=tf.stack(\n",
        "                [tf.range(batch_size, dtype=tf.int64), tf.zeros(batch_size, dtype=tf.int64), start_label_indices],\n",
        "                axis=1\n",
        "            )\n",
        "        )\n",
        "\n",
        "        score = start_transition_scores + first_emission_scores\n",
        "\n",
        "        for t in range(1, seq_len):\n",
        "            prev_labels = labels[:, t - 1]\n",
        "            curr_labels = labels[:, t]\n",
        "\n",
        "            indices = tf.stack([prev_labels, curr_labels], axis=1)\n",
        "            transition_scores = tf.gather_nd(self.transition_matrix, indices)\n",
        "            curr_emission_scores = tf.gather_nd(\n",
        "                emissions,\n",
        "                indices=tf.stack(\n",
        "                    [tf.range(batch_size, dtype=tf.int64), tf.cast(tf.fill([batch_size], t), tf.int64), curr_labels],\n",
        "                    axis=1\n",
        "                )\n",
        "            )\n",
        "\n",
        "            score += transition_scores + curr_emission_scores\n",
        "\n",
        "        end_label_indices = labels[:, seq_len - 1]\n",
        "        end_transition_scores = tf.gather(self.end_transitions, end_label_indices)\n",
        "\n",
        "        score += end_transition_scores\n",
        "\n",
        "        return score\n",
        "\n",
        "    @tf.function\n",
        "    def viterbi(self, emissions, attention_mask):\n",
        "        batch_size = tf.shape(emissions)[0]\n",
        "        seq_len = tf.shape(emissions)[1]\n",
        "        num_tags = tf.shape(emissions)[2]\n",
        "\n",
        "        dp = tf.TensorArray(dtype=tf.float32, size=seq_len, clear_after_read=False)\n",
        "        backpointer = tf.TensorArray(dtype=tf.int32, size=seq_len, clear_after_read=False)\n",
        "\n",
        "        first_step = self.start_transitions + emissions[:, 0, :]\n",
        "        dp = dp.write(0, first_step)\n",
        "\n",
        "        t = tf.constant(1)\n",
        "\n",
        "        def loop_body(t, dp, backpointer):\n",
        "            prev_scores = dp.read(t - 1)\n",
        "            scores = tf.expand_dims(prev_scores, axis=2) + self.transition_matrix\n",
        "            best_scores = tf.reduce_max(scores, axis=1)\n",
        "            best_paths = tf.argmax(scores, axis=1, output_type=tf.int32)\n",
        "            current_scores = emissions[:, t, :] + best_scores\n",
        "            dp = dp.write(t, current_scores)\n",
        "            backpointer = backpointer.write(t, best_paths)\n",
        "            return t + 1, dp, backpointer\n",
        "\n",
        "        _, dp, backpointer = tf.while_loop(\n",
        "            cond=lambda t, *_: t < seq_len,\n",
        "            body=loop_body,\n",
        "            loop_vars=(t, dp, backpointer)\n",
        "        )\n",
        "\n",
        "        last_step_scores = dp.read(seq_len - 1) + self.end_transitions\n",
        "        last_tag = tf.argmax(last_step_scores, axis=1, output_type=tf.int32)\n",
        "\n",
        "        def backtrack_fn(i):\n",
        "            best_path = tf.TensorArray(dtype=tf.int32, size=seq_len, clear_after_read=False)\n",
        "            best_path = best_path.write(seq_len - 1, last_tag[i])\n",
        "            t = seq_len - 2\n",
        "\n",
        "            def backtrack_body(t, best_path):\n",
        "                next_tag = best_path.read(t + 1)\n",
        "                best_tag = backpointer.read(t + 1)[i, next_tag]\n",
        "                best_path = best_path.write(t, best_tag)\n",
        "                return t - 1, best_path\n",
        "\n",
        "            _, best_path = tf.while_loop(\n",
        "                cond=lambda t, *_: t >= 0,\n",
        "                body=backtrack_body,\n",
        "                loop_vars=(t, best_path)\n",
        "            )\n",
        "\n",
        "            return best_path.stack()\n",
        "\n",
        "        best_paths = tf.map_fn(backtrack_fn, tf.range(batch_size), fn_output_signature=tf.int32)\n",
        "        return best_paths\n",
        "\n",
        "\n",
        "class NERREModel(tf.keras.Model):\n",
        "    def __init__(self, num_tags, hidden_dim, seq_len, num_relations, ner_lr=1e-3, re_lr=1e-4, **kwargs):\n",
        "        super(NERREModel, self).__init__(**kwargs)\n",
        "        self.bert = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "        self.lstm = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hidden_dim, return_sequences=True))\n",
        "        self.lstm.build((None, None, self.bert.config.hidden_size))\n",
        "\n",
        "        self.dense_ner = tf.keras.layers.Dense(num_tags)\n",
        "        self.dense_ner.build((None, 2*hidden_dim))\n",
        "\n",
        "        self.crf = CustomCRF(num_tags, seq_len)\n",
        "\n",
        "        self.dense1_re = tf.keras.layers.Dense(384, activation='relu')\n",
        "        self.dense1_re.build((None, 4*hidden_dim))\n",
        "        self.dense2_re = tf.keras.layers.Dense(128, activation='relu')\n",
        "        self.dense2_re.build((None, 384))\n",
        "        self.dense3_re = tf.keras.layers.Dense(num_relations, activation='softmax')\n",
        "        self.dense3_re.build((None, 128))\n",
        "\n",
        "        self.ner_optimizer = tf.keras.optimizers.Adam(learning_rate=ner_lr)\n",
        "        self.re_optimizer = tf.keras.optimizers.Adam(learning_rate=re_lr)\n",
        "        self.num_tags = num_tags\n",
        "        self.num_relations = num_relations\n",
        "        self.ner_metrics_dict = {i: tf.Variable([0, 0, 0, 0], dtype=tf.int32) for i in range(num_tags)}\n",
        "        self.re_metrics_dict = {i: tf.Variable([0, 0, 0, 0], dtype=tf.int32) for i in range(num_relations)}\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.crf.build(input_shape)\n",
        "\n",
        "    def reset_metrics(self):\n",
        "        for key in self.re_metrics_dict:\n",
        "            self.re_metrics_dict[key].assign([0, 0, 0, 0])\n",
        "        for key in self.ner_metrics_dict:\n",
        "            self.ner_metrics_dict[key].assign([0, 0, 0, 0])\n",
        "\n",
        "    @tf.function\n",
        "    def extract_entity_pairs(self, lstm_output, ner_tags, re_mask):\n",
        "        combined_embeddings = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "        batch_size = tf.shape(ner_tags)[0]\n",
        "\n",
        "        def process_batch(i, embeddings_list):\n",
        "            re_pairs = re_mask[i]\n",
        "            valid_pairs = tf.boolean_mask(re_pairs, tf.not_equal(re_pairs[:, 0], -1))\n",
        "\n",
        "            def process_pair(j, embeddings_list):\n",
        "                e1_sidx, e1_eidx, relation_type, e2_sidx, e2_eidx = tf.unstack(valid_pairs[j])\n",
        "                e1_emb = self.pool_entity(lstm_output[i], ner_tags[i], e1_sidx, e1_eidx)\n",
        "                e2_emb = self.pool_entity(lstm_output[i], ner_tags[i], e2_sidx, e2_eidx)\n",
        "                combined = tf.concat([e1_emb, e2_emb], axis=-1)\n",
        "                return j + 1, embeddings_list.write(embeddings_list.size(), combined)\n",
        "\n",
        "            _, embeddings_list = tf.while_loop(\n",
        "                lambda j, _: j < tf.shape(valid_pairs)[0],\n",
        "                process_pair,\n",
        "                loop_vars=[0, embeddings_list]\n",
        "            )\n",
        "            return i + 1, embeddings_list\n",
        "\n",
        "        _, final_embeddings = tf.while_loop(\n",
        "            lambda i, _: i < batch_size,\n",
        "            process_batch,\n",
        "            loop_vars=[0, combined_embeddings]\n",
        "        )\n",
        "\n",
        "        return final_embeddings.stack()\n",
        "    @tf.function\n",
        "    def pool_entity(self, lstm_output, ner_tags, start_idx, end_idx):\n",
        "        entity_span = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
        "\n",
        "        def condition(idx, _):\n",
        "            return idx <= end_idx\n",
        "\n",
        "        def body(idx, entity_span):\n",
        "            entity_span = entity_span.write(entity_span.size(), lstm_output[idx])\n",
        "            return idx + 1, entity_span\n",
        "\n",
        "        _, collected_span = tf.while_loop(\n",
        "            condition, body, loop_vars=[start_idx, entity_span]\n",
        "        )\n",
        "\n",
        "        return tf.reduce_mean(collected_span.stack(), axis=0)\n",
        "\n",
        "    @tf.function\n",
        "    def extract_relation_labels(self, re_mask):\n",
        "        reduced_re_mask = re_mask[:, :, 2]\n",
        "        valid_mask = tf.not_equal(reduced_re_mask, -1)\n",
        "        valid_relation_labels = tf.boolean_mask(reduced_re_mask, valid_mask)\n",
        "        return valid_relation_labels\n",
        "    @tf.function\n",
        "    def call(self, inputs, labels=None, training=None):\n",
        "        bert_output = self.bert(inputs['input_ids'], attention_mask=inputs['attention_mask'])['last_hidden_state']\n",
        "        lstm_output = self.lstm(bert_output)\n",
        "        ner_logits = self.dense_ner(lstm_output)\n",
        "        re_embeddings = self.extract_entity_pairs(lstm_output, inputs['ner_tags'], inputs['re_mask'])\n",
        "        combined_embeddings_stacked = tf.stack(re_embeddings)\n",
        "\n",
        "        re_logits = self.dense3_re(self.dense2_re(self.dense1_re(combined_embeddings_stacked)))\n",
        "\n",
        "        if training:\n",
        "            return ner_logits, re_logits\n",
        "        else:\n",
        "            return self.crf((ner_logits, inputs['attention_mask']), training=False), re_logits\n",
        "    @tf.function\n",
        "    def train_step(self, data):\n",
        "        inputs, labels = data\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            ner_logits, re_logits = self(inputs, training=True)\n",
        "            ner_loss = self.crf._crf_loss(ner_logits, labels, inputs['attention_mask'], tf.shape(ner_logits)[1])\n",
        "            filtered_relation_labels = self.extract_relation_labels(inputs['re_mask'])\n",
        "            re_loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(\n",
        "                filtered_relation_labels, re_logits, from_logits=False\n",
        "            ))\n",
        "\n",
        "        ner_trainable_vars = [self.lstm.trainable_variables, self.dense_ner.trainable_variables, self.crf.trainable_variables]\n",
        "        ner_trainable_vars = [var for sublist in ner_trainable_vars for var in sublist]\n",
        "        re_trainable_vars = [self.lstm.trainable_variables, self.dense1_re.trainable_variables, self.dense2_re.trainable_variables, self.dense3_re.trainable_variables]\n",
        "        re_trainable_vars = [var for sublist in re_trainable_vars for var in sublist]\n",
        "\n",
        "        ner_gradients = tape.gradient(ner_loss, ner_trainable_vars)\n",
        "        self.ner_optimizer.apply_gradients(zip(ner_gradients, ner_trainable_vars))\n",
        "        re_gradients = tape.gradient(re_loss, re_trainable_vars)\n",
        "        self.re_optimizer.apply_gradients(zip(re_gradients, re_trainable_vars))\n",
        "\n",
        "        del tape\n",
        "\n",
        "        return {\"ner_loss\": ner_loss, \"re_loss\": re_loss}\n",
        "    def test_step(self, data, validation = False):\n",
        "        inputs, labels = data\n",
        "\n",
        "        if validation:\n",
        "            ner_logits, re_logits = self(inputs, training=True)\n",
        "            ner_loss = self.crf._crf_loss(ner_logits, labels, inputs['attention_mask'], tf.shape(ner_logits)[1])\n",
        "            filtered_relation_labels = self.extract_relation_labels(inputs['re_mask'])\n",
        "            re_loss = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(\n",
        "                filtered_relation_labels, re_logits, from_logits=False\n",
        "            ))\n",
        "            return {\"ner_loss\": ner_loss, \"re_loss\": re_loss}\n",
        "\n",
        "        emissions, re_logits = self(inputs, training=False)\n",
        "        predictions = tf.reshape(emissions, [-1])\n",
        "        true_labels = tf.reshape(labels, [-1])\n",
        "\n",
        "        for i in range(self.num_tags):\n",
        "            true_positives = tf.reduce_sum(tf.cast((predictions == i) & (true_labels == i), tf.int32))\n",
        "            false_positives = tf.reduce_sum(tf.cast((predictions == i) & (true_labels != i), tf.int32))\n",
        "            false_negatives = tf.reduce_sum(tf.cast((predictions != i) & (true_labels == i), tf.int32))\n",
        "            true_negatives = tf.reduce_sum(tf.cast((predictions != i) & (true_labels != i), tf.int32))\n",
        "\n",
        "            self.ner_metrics_dict[i].assign_add([true_positives, true_negatives, false_positives, false_negatives])\n",
        "\n",
        "        predictions = tf.argmax(re_logits, axis=-1)\n",
        "        true_labels = self.extract_relation_labels(inputs['re_mask'])\n",
        "\n",
        "        for i in range(self.num_relations):\n",
        "            tp = tf.reduce_sum(tf.cast((predictions == i) & (true_labels == i), tf.int32))\n",
        "            fp = tf.reduce_sum(tf.cast((predictions == i) & (true_labels != i), tf.int32))\n",
        "            fn = tf.reduce_sum(tf.cast((predictions != i) & (true_labels == i), tf.int32))\n",
        "            tn = tf.reduce_sum(tf.cast((predictions != i) & (true_labels != i), tf.int32))\n",
        "\n",
        "            self.re_metrics_dict[i].assign_add([tp, tn, fp, fn])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSrF6HEYcLdB",
        "outputId": "186b43db-b297-4e99-f151-055fcdf8dfd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import random\n",
        "from transformers import AutoTokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "ds = load_dataset(\"bigbio/chemprot\", \"chemprot_full_source\")\n",
        "\n",
        "def gen(i, j, ner_tags_segment, next_tags):\n",
        "    l, m = i + 1, j + 1\n",
        "    tem = []\n",
        "    while ner_tags_segment[l] in next_tags[ner_tags_segment[i]][0]:\n",
        "        l += 1\n",
        "    while ner_tags_segment[m] in next_tags[ner_tags_segment[j]][0]:\n",
        "        m += 1\n",
        "    if m - 1 != j and l - 1 != i:\n",
        "        tem.append((i, l - 1, j, m - 1))\n",
        "    l, m = i + 1, j + 1\n",
        "    while ner_tags_segment[l] in next_tags[ner_tags_segment[i]][0]:\n",
        "        l += 1\n",
        "    while ner_tags_segment[m] in next_tags[ner_tags_segment[j]][1]:\n",
        "        m += 1\n",
        "    if m - 1 != j and l - 1 != i:\n",
        "        tem.append((i, l - 1, j, m - 1))\n",
        "    l, m = i + 1, j + 1\n",
        "    while ner_tags_segment[l] in next_tags[ner_tags_segment[i]][1]:\n",
        "        l += 1\n",
        "    while ner_tags_segment[m] in next_tags[ner_tags_segment[j]][0]:\n",
        "        m += 1\n",
        "    if m - 1 != j and l - 1 != i:\n",
        "        tem.append((i, l - 1, j, m - 1))\n",
        "    l, m = i + 1, j + 1\n",
        "    while ner_tags_segment[l] in next_tags[ner_tags_segment[i]][1]:\n",
        "        l += 1\n",
        "    while ner_tags_segment[m] in next_tags[ner_tags_segment[j]][1]:\n",
        "        m += 1\n",
        "    if m - 1 != j and l - 1 != i:\n",
        "        tem.append((i, l - 1, j, m - 1))\n",
        "    return tem\n",
        "\n",
        "def prepare_data(row, tag_dict, r_dict, tag_dict_map, next_tags, max_length=128, stride=10, padding=True):\n",
        "    text = row['text']\n",
        "    entities = row['entities']\n",
        "    relations = row['relations']\n",
        "\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        return_offsets_mapping=True,\n",
        "        return_overflowing_tokens=True,\n",
        "        max_length=max_length,\n",
        "        truncation=True,\n",
        "        stride=stride,\n",
        "        padding='max_length' if padding else False\n",
        "    )\n",
        "\n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    ner_tags = []\n",
        "    relation_mask = []\n",
        "\n",
        "    for i, offset_mapping in enumerate(tokenized_inputs['offset_mapping']):\n",
        "        input_ids_segment = tokenized_inputs['input_ids'][i]\n",
        "        attention_mask_segment = tokenized_inputs['attention_mask'][i]\n",
        "\n",
        "        ner_tags_segment = [(0,) for i in range(max_length)]\n",
        "        rel_mask_segment = []\n",
        "        entity_map = {}\n",
        "        b_tag_indices = []\n",
        "\n",
        "        for entity_id, entity_type, (start_char, end_char) in zip(entities['id'], entities['type'], entities['offsets']):\n",
        "            entity_start_found = False\n",
        "            for token_idx, (token_start, token_end) in enumerate(offset_mapping):\n",
        "                if token_idx == 0 or token_idx == max_length - 1:\n",
        "                    continue\n",
        "                if token_start == start_char and offset_mapping[-2][1] >= end_char:\n",
        "                    ner_tags_segment[token_idx] += (tag_dict[f'B-{entity_type}'],)\n",
        "                    entity_map[entity_id] = [token_idx, token_idx]\n",
        "                    entity_start_found = True\n",
        "                elif entity_start_found and token_start < end_char:\n",
        "                    ner_tags_segment[token_idx] += (tag_dict[f'I-{entity_type}'],)\n",
        "                    entity_map[entity_id][1] = token_idx\n",
        "\n",
        "        rel_pairs = set()\n",
        "        for rel_type, arg1_id, arg2_id in zip(relations['type'], relations['arg1'], relations['arg2']):\n",
        "            if arg1_id in entity_map and arg2_id in entity_map:\n",
        "                rel_mask_segment.append([entity_map[arg1_id][0], entity_map[arg1_id][1], r_dict[rel_type], entity_map[arg2_id][0], entity_map[arg2_id][1]])\n",
        "                rel_pairs.add((entity_map[arg1_id][0], entity_map[arg1_id][1], entity_map[arg2_id][0], entity_map[arg2_id][1]))\n",
        "\n",
        "        for j in range(max_length):\n",
        "            ner_tags_segment[j] = tag_dict_map[tuple(sorted(list(set(ner_tags_segment[j]))))]\n",
        "            if ner_tags_segment[j] in {1, 2, 4, 7, 8, 12, 13, 15, 16, 17, 18}:\n",
        "                b_tag_indices.append(j)\n",
        "\n",
        "        relpairs = set(rel_pairs)\n",
        "        neg_samples = []\n",
        "        for i in rel_pairs:\n",
        "            for j in b_tag_indices:\n",
        "                tem = gen(i[0], j, ner_tags_segment, next_tags)\n",
        "                for ke in tem:\n",
        "                    if ke not in relpairs:\n",
        "                        relpairs.add(ke)\n",
        "                        neg_samples.append([ke[0], ke[1], 11, ke[2], ke[3]])\n",
        "                tem = gen(j, i[2], ner_tags_segment, next_tags)\n",
        "                for ke in tem:\n",
        "                    if ke not in relpairs:\n",
        "                        relpairs.add(ke)\n",
        "                        neg_samples.append([ke[0], ke[1], 11, ke[2], ke[3]])\n",
        "\n",
        "        neg_samples = random.sample(neg_samples, len(rel_pairs)) if len(rel_pairs) != 0 and len(rel_pairs) <= len(neg_samples) else random.sample(neg_samples, 15) if len(neg_samples) >= 15 else neg_samples\n",
        "\n",
        "        neg_samples_r = []\n",
        "        for i in b_tag_indices:\n",
        "            for j in b_tag_indices:\n",
        "                tem = gen(i, j, ner_tags_segment, next_tags)\n",
        "                for ke in tem:\n",
        "                    if ke not in relpairs:\n",
        "                        relpairs.add(ke)\n",
        "                        neg_samples_r.append([ke[0], ke[1], 11, ke[2], ke[3]])\n",
        "\n",
        "        neg_samples_r = random.sample(neg_samples_r, len(rel_pairs)) if len(rel_pairs) != 0 and len(rel_pairs) <= len(neg_samples_r) else random.sample(neg_samples_r, 15) if len(neg_samples_r) >= 15 else neg_samples_r\n",
        "        input_ids.append(input_ids_segment)\n",
        "        attention_mask.append(attention_mask_segment)\n",
        "        ner_tags.append(ner_tags_segment)\n",
        "        relation_mask.append(rel_mask_segment + neg_samples + neg_samples_r)\n",
        "\n",
        "    return input_ids, attention_mask, ner_tags, relation_mask\n",
        "\n"
      ],
      "metadata": {
        "id": "uNoukqPFOtBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgh8N3NprTMA"
      },
      "outputs": [],
      "source": [
        "tag_dict = {\n",
        "    'B-GENE-Y': 1, 'I-GENE-Y': 2,\n",
        "    'B-GENE-N': 3, 'I-GENE-N': 4,\n",
        "    'B-CHEMICAL': 5, 'I-CHEMICAL': 6\n",
        "}\n",
        "\n",
        "tag_dict_map = {\n",
        "    (0,): 0, (0, 1): 1, (0, 2, 3, 5): 2,\n",
        "    (0, 4): 3, (0, 1, 6): 4, (0, 2, 6): 5, (0, 2): 6, (0, 3, 6): 7,\n",
        "    (0, 5): 8, (0, 4, 6): 9, (0, 2, 4, 6): 10, (0, 2, 4): 11,\n",
        "    (0, 5, 6): 12, (0, 3): 13, (0, 6): 14, (0, 1, 5): 15, (0, 2, 5): 16, (0, 3, 5): 17, (0, 4, 5): 18\n",
        "}\n",
        "\n",
        "con = {2:[2,5,6,10,11,16], 4:[3,9,10,11,18], 6:[4,5,7,9,10,12,14]}\n",
        "\n",
        "next_entity = {0:[[0],[]], 1:[con[2],[]], 2:[con[4],con[6]], 3:[[3],[]], 4:[con[2],[]], 5:[[5],[]], 6:[[6],[]], 7:[con[4],[]], 8:[con[6],[]], 9:[[9],[]] ,10:[[10],[]], 11:[[11],[]], 12:[con[6],[]], 13:[con[4],[]], 14:[[14],[]], 15:[con[2],con[6]], 16:[con[6],[]], 17:[con[4],con[6]], 18:[con[6],[]]}\n",
        "\n",
        "r_dict = {\n",
        "    'CPR:0': 0, 'CPR:1': 1, 'CPR:2': 2, 'CPR:3': 3,\n",
        "    'CPR:4': 4, 'CPR:5': 5, 'CPR:6': 6, 'CPR:7': 7,\n",
        "    'CPR:8': 8, 'CPR:9': 9, 'CPR:10': 10\n",
        "}\n",
        "\n",
        "all_input_ids, all_attention_masks, all_ner_tags, all_relation_masks = [], [], [], []\n",
        "for row in ds['train']:\n",
        "    input_ids, attention_mask, ner_tags, relation_mask = prepare_data(row, tag_dict, r_dict, tag_dict_map, next_entity)\n",
        "    all_input_ids.extend(input_ids)\n",
        "    all_attention_masks.extend(attention_mask)\n",
        "    all_ner_tags.extend(ner_tags)\n",
        "    all_relation_masks.extend(relation_mask)\n",
        "\n",
        "prepared_data = {\n",
        "    'input_ids': np.array(all_input_ids),\n",
        "    'attention_mask': np.array(all_attention_masks),\n",
        "    'ner_tags': np.array(all_ner_tags),\n",
        "    're_mask': all_relation_masks\n",
        "}\n",
        "\n",
        "hidden_dim = 128\n",
        "num_tags = 19\n",
        "seq_len = 128\n",
        "num_relations = 12\n",
        "\n",
        "padded_relation_mask = pad_sequences(\n",
        "    [np.array(item, dtype=int) for item in prepared_data['re_mask']],\n",
        "    padding='post',\n",
        "    value=-1,\n",
        "    dtype='object'\n",
        ")\n",
        "\n",
        "padded_relation_mask = np.array(padded_relation_mask, dtype=int)\n",
        "\n",
        "inputs = {\n",
        "    'input_ids': prepared_data['input_ids'],\n",
        "    'attention_mask': prepared_data['attention_mask'],\n",
        "    're_mask': padded_relation_mask,\n",
        "    'ner_tags': prepared_data['ner_tags']\n",
        "}\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': inputs['input_ids'],\n",
        "        'attention_mask': inputs['attention_mask'],\n",
        "        're_mask': padded_relation_mask,\n",
        "        'ner_tags': inputs['ner_tags']\n",
        "    },\n",
        "    inputs['ner_tags']\n",
        "))\n",
        "\n",
        "\n",
        "vall_input_ids, vall_attention_masks, vall_ner_tags, vall_relation_masks = [], [], [], []\n",
        "for row in ds['validation']:\n",
        "    input_ids, attention_mask, ner_tags, relation_mask = prepare_data(row, tag_dict, r_dict, tag_dict_map, next_entity)\n",
        "    vall_input_ids.extend(input_ids)\n",
        "    vall_attention_masks.extend(attention_mask)\n",
        "    vall_ner_tags.extend(ner_tags)\n",
        "    vall_relation_masks.extend(relation_mask)\n",
        "\n",
        "vprepared_data = {\n",
        "    'input_ids': np.array(vall_input_ids),\n",
        "    'attention_mask': np.array(vall_attention_masks),\n",
        "    'ner_tags': np.array(vall_ner_tags),\n",
        "    're_mask': vall_relation_masks\n",
        "}\n",
        "\n",
        "\n",
        "vpadded_relation_mask = pad_sequences(\n",
        "    [np.array(item, dtype=int) for item in vprepared_data['re_mask']],\n",
        "    padding='post',\n",
        "    value=-1,\n",
        "    dtype='object'\n",
        ")\n",
        "\n",
        "vpadded_relation_mask = np.array(vpadded_relation_mask, dtype=int)\n",
        "\n",
        "vinputs = {\n",
        "    'input_ids': vprepared_data['input_ids'],\n",
        "    'attention_mask': vprepared_data['attention_mask'],\n",
        "    're_mask': vpadded_relation_mask,\n",
        "    'ner_tags': vprepared_data['ner_tags']\n",
        "}\n",
        "\n",
        "vdataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': vinputs['input_ids'],\n",
        "        'attention_mask': vinputs['attention_mask'],\n",
        "        're_mask': vpadded_relation_mask,\n",
        "        'ner_tags': vinputs['ner_tags']\n",
        "    },\n",
        "    vinputs['ner_tags']\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7uCiwb9SU9k4"
      },
      "outputs": [],
      "source": [
        "\n",
        "nerre_model = NERREModel(num_tags, hidden_dim, seq_len, num_relations)\n",
        "nerre_model.compile()\n",
        "batch_size = 30\n",
        "dataset = dataset.batch(batch_size)\n",
        "vdataset = vdataset.batch(batch_size)\n",
        "num_epochs = 20\n",
        "num_batches = 131\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    train_ner_loss = 0\n",
        "    val_ner_loss = 0\n",
        "    train_re_loss = 0\n",
        "    val_re_loss = 0\n",
        "\n",
        "    t_b, v_b = 0, 0\n",
        "    for batch_num, batch_data in enumerate(dataset.take(num_batches)):\n",
        "        metrics = nerre_model.train_step(batch_data)\n",
        "        train_ner_loss += metrics['ner_loss']\n",
        "        train_re_loss += metrics['re_loss']\n",
        "        t_b += 1\n",
        "    for batch_num, batch_data in enumerate(vdataset.take(num_batches)):\n",
        "        metrics = nerre_model.test_step(batch_data, validation = True)\n",
        "        val_ner_loss += metrics['ner_loss']\n",
        "        val_re_loss += metrics['re_loss']\n",
        "        v_b += 1\n",
        "\n",
        "    print(f\"End of Epoch {epoch + 1}, train NER loss: {train_ner_loss / t_b} train RE loss: {train_re_loss / t_b} validation NER loss: {val_ner_loss / v_b} validation RE loss: {val_re_loss / v_b}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lEWkZGEXYiO"
      },
      "outputs": [],
      "source": [
        "\n",
        "all_input_ids, all_attention_masks, all_ner_tags, all_relation_masks = [], [], [], []\n",
        "for row in ds['test']:\n",
        "    input_ids, attention_mask, ner_tags, relation_mask = prepare_data(row, tag_dict, r_dict, tag_dict_map, next_entity)\n",
        "    all_input_ids.extend(input_ids)\n",
        "    all_attention_masks.extend(attention_mask)\n",
        "    all_ner_tags.extend(ner_tags)\n",
        "    all_relation_masks.extend(relation_mask)\n",
        "\n",
        "prepared_data = {\n",
        "    'input_ids': np.array(all_input_ids),\n",
        "    'attention_mask': np.array(all_attention_masks),\n",
        "    'ner_tags': np.array(all_ner_tags),\n",
        "    're_mask': all_relation_masks\n",
        "}\n",
        "\n",
        "padded_relation_mask = pad_sequences(\n",
        "    [np.array(item, dtype=int) for item in prepared_data['re_mask']],\n",
        "    padding='post',\n",
        "    value=-1,\n",
        "    dtype='object'\n",
        ")\n",
        "\n",
        "padded_relation_mask = np.array(padded_relation_mask, dtype=int)\n",
        "\n",
        "inputs = {\n",
        "    'input_ids': prepared_data['input_ids'],\n",
        "    'attention_mask': prepared_data['attention_mask'],\n",
        "    're_mask': padded_relation_mask,\n",
        "    'ner_tags': prepared_data['ner_tags']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfQeQ6tjXYuV"
      },
      "outputs": [],
      "source": [
        "nerre_model.reset_metrics()\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'input_ids': inputs['input_ids'],\n",
        "        'attention_mask': inputs['attention_mask'],\n",
        "        're_mask': padded_relation_mask,\n",
        "        'ner_tags': inputs['ner_tags']\n",
        "    },\n",
        "    inputs['ner_tags']\n",
        "))\n",
        "dataset = dataset.batch(batch_size)\n",
        "\n",
        "for batch_num, batch_data in enumerate(dataset.take(num_batches)):\n",
        "    nerre_model.test_step(batch_data)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = '/content/nerre_model.keras'\n",
        "nerre_model.save(model_save_path)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(model_save_path)\n"
      ],
      "metadata": {
        "id": "g_x4k_CGPxMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TzC1jhmV88s"
      },
      "outputs": [],
      "source": [
        "\n",
        "def calculate_final_metrics(num_classes, threshold, metrics_dict):\n",
        "    avg_precision, avg_recall, avg_f1_score = [], [], []\n",
        "    total_tp, total_fp, total_fn, total_tn = 0, 0, 0, 0\n",
        "    tpa,fpa,tna,fna,k=[],[],[],[],0\n",
        "    for i in range(num_classes):\n",
        "        tp, tn, fp, fn = metrics_dict[i].numpy()\n",
        "        if tp+fn < threshold:\n",
        "            continue\n",
        "\n",
        "        k += 1\n",
        "        tpa.append(tp)\n",
        "        fpa.append(fp)\n",
        "        tna.append(tn)\n",
        "        fna.append(fn)\n",
        "\n",
        "        total_tp += tp\n",
        "        total_fp += fp\n",
        "        total_fn += fn\n",
        "        total_tn += tn\n",
        "\n",
        "        precision = tp / (tp + fp + tf.keras.backend.epsilon())\n",
        "        recall = tp / (tp + fn + tf.keras.backend.epsilon())\n",
        "        f1 = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
        "\n",
        "        avg_precision.append(precision)\n",
        "        avg_recall.append(recall)\n",
        "        avg_f1_score.append(f1)\n",
        "\n",
        "    accuracy = (total_tp ) / (total_tp + total_fp + tf.keras.backend.epsilon())\n",
        "    return {\n",
        "        \"average_precision\": tf.reduce_mean(avg_precision),\n",
        "        \"average_recall\": tf.reduce_mean(avg_recall),\n",
        "        \"average_f1_score\": tf.reduce_mean(avg_f1_score),\n",
        "        \"accuracy\": accuracy\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca1xbdHCZacg"
      },
      "outputs": [],
      "source": [
        "print('NER task metrics')\n",
        "ner_metrics = calculate_final_metrics(19, 10, nerre_model.ner_metrics_dict)\n",
        "for key in ner_metrics.keys():\n",
        "    print(f\"{key}: {ner_metrics[key]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApdZ8heaZdOo"
      },
      "outputs": [],
      "source": [
        "print('RE task metrics')\n",
        "re_metrics = calculate_final_metrics(12, 30, nerre_model.re_metrics_dict)\n",
        "for key in re_metrics.keys():\n",
        "    print(f\"{key}: {re_metrics[key]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yRVE9QJ6e6sT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}